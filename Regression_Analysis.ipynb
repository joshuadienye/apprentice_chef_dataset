{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to time the program\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing file path as file\n",
    "file = './Apprentice_Chef_Dataset.xlsx'\n",
    "\n",
    "# creating dataset using the file path\n",
    "ac_dataset = pd.read_excel(io = file)\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all column names to lowercase\n",
    "ac_dataset.columns = ac_dataset.columns.str.lower()\n",
    "\n",
    "# checking information of all columns\n",
    "#ac_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing first five rows of dataset\n",
    "#ac_dataset.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ac_dataset.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Continuous variables\n",
    "-------------------------------\n",
    "revenue\n",
    "avg_prep_vid_time\n",
    "avg_time_per_site_visit\n",
    "\n",
    "Categorical variables\n",
    "-------------------------------\n",
    "cross_sell_success\n",
    "email\n",
    "package_locker\t\n",
    "refrigerated_locker\n",
    "tastes_and_preferences\n",
    "mobile_number\n",
    "\n",
    "Count variables\n",
    "-------------------------------\n",
    "avg_clicks_per_visit\t\n",
    "total_photos_viewed\n",
    "master_classes_attended\t\n",
    "median_meal_rating\n",
    "largest_order_size\n",
    "pc_logins\t\n",
    "mobile_logins\t\n",
    "weekly_plan\tearly_deliveries\t\n",
    "late_deliveries\n",
    "cancellations_before_noon\t\n",
    "cancellations_after_noon\n",
    "total_meals_ordered\t\n",
    "unique_meals_purch\t\n",
    "contacts_w_customer_service\t\n",
    "product_categories_viewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if any columns have missing values \n",
    "#ac_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the rows with null values in column family_name\n",
    "#ac_dataset[ac_dataset['family_name'].isnull()].head(n = 5)\n",
    "\n",
    "# The family_name probably didn't get taken because they are in brackets\n",
    "# Nothing interesting of note can be found and family name does not affect our\n",
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns name, first_name, and family_name\n",
    "ac_dataset = ac_dataset.drop(['name', 'first_name', 'family_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking new dataset column names\n",
    "#ac_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for any null values\n",
    "#print(ac_dataset.isnull().any().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking into the continuous variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the plot for 'Mas Vnr Area'\n",
    "#sns.displot(x = 'revenue',\n",
    "#            data = ac_dataset,\n",
    "#            height = 5,\n",
    "#            aspect = 2)\n",
    "\n",
    "\n",
    "# title and labels\n",
    "#plt.title('Distribution of Revenue')\n",
    "\n",
    "# displaying the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our response variable is skewed and has outliers. To fix this, we should should perform a log transformation to bring the values closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformation of revenue and saving as new column\n",
    "ac_dataset['log_revenue'] = np.log10(ac_dataset['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the plot for 'Mas Vnr Area'\n",
    "#sns.displot(x = 'log_revenue',\n",
    "#            data = ac_dataset,\n",
    "#            height = 5,\n",
    "#            aspect = 2)\n",
    "\n",
    "\n",
    "# title and labels\n",
    "#plt.title('Distribution of Log of Revenue')\n",
    "\n",
    "# displaying the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Visual EDA (Scatterplots)\n",
    "########################\n",
    "\n",
    "# setting figure size\n",
    "#fig, ax = plt.subplots(figsize = (10, 8))\n",
    "\n",
    "\n",
    "# developing a scatterplot\n",
    "#plt.subplot(2, 2, 1)\n",
    "#sns.scatterplot(x = ac_dataset['avg_prep_vid_time'],\n",
    "#                y = ac_dataset['revenue'],\n",
    "#                color = 'b')\n",
    "\n",
    "# adding labels but not adding title\n",
    "# plt.xlabel(xlabel = 'Average Time Prep Instruction Video Played (Seconds)')\n",
    "# plt.ylabel(ylabel = 'Revenue')\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "# developing a scatterplot\n",
    "# plt.subplot(2, 2, 2)\n",
    "# sns.scatterplot(x = ac_dataset['avg_time_per_site_visit'],\n",
    "#                 y = ac_dataset['revenue'],\n",
    "#                 color = 'r')\n",
    "\n",
    "# adding labels but not adding title\n",
    "# plt.xlabel(xlabel = 'Average Time Spent Per Web/Mobile Visit')\n",
    "# plt.ylabel(ylabel = 'Revenue')\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "# cleaning up the layout, saving the figures, and displaying the results\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranforming the variables to log\n",
    "ac_dataset['log_avg_time_per_site_visit'] = np.log10\\\n",
    "                                            (ac_dataset['avg_time_per_site_visit'])\n",
    "\n",
    "ac_dataset['log_avg_prep_vid_time'] = np.log10\\\n",
    "                                            (ac_dataset['avg_prep_vid_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Visual EDA (Scatterplots)\n",
    "########################\n",
    "\n",
    "# setting figure size\n",
    "# fig, ax = plt.subplots(figsize = (10, 8))\n",
    "\n",
    "\n",
    "# developing a scatterplot\n",
    "# plt.subplot(2, 2, 1)\n",
    "# sns.scatterplot(x = ac_dataset['log_avg_prep_vid_time'],\n",
    "#                 y = ac_dataset['log_revenue'],\n",
    "#                 color = 'b')\n",
    "\n",
    "# adding labels but not adding title\n",
    "# plt.xlabel(xlabel = 'Log Average Time Prep Instruction Video Played (Seconds)')\n",
    "# plt.ylabel(ylabel = 'Log Revenue')\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "# developing a scatterplot\n",
    "# plt.subplot(2, 2, 2)\n",
    "# sns.scatterplot(x = ac_dataset['log_avg_time_per_site_visit'],\n",
    "#                 y = ac_dataset['log_revenue'],\n",
    "#                 color = 'r')\n",
    "\n",
    "# adding labels but not adding title\n",
    "# plt.xlabel(xlabel = 'Log Average Time Spent Per Web/Mobile Visit')\n",
    "# plt.ylabel(ylabel = 'Log Revenue')\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "# cleaning up the layout, saving the figures, and displaying the results\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking into the categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the marketing team is very adamant about email classification so emails need\n",
    "# to be classsified as requested\n",
    "\n",
    "# creating lists for different categories\n",
    "prof_email = ['mmm', 'amex', 'apple', 'boeing', 'caterpillar', 'chevron',\n",
    "             'cisco', 'cocacola', 'disney', 'dupont', 'exxon', 'ge', 'walmart',\n",
    "             'goldmansacs', 'homedepot', 'ibm', 'intel', 'jnj', 'jpmorgan',\n",
    "             'mcdonalds', 'merck', 'microsoft', 'nike', 'pfizer', 'pg',\n",
    "             'travelers', 'unitedtech', 'unitedhealth', 'verizon', 'visa']\n",
    "per_email = ['gmail', 'yahoo', 'protonmail']\n",
    "junk_email = ['me', 'aol', 'hotmail', 'live', 'msn', 'passport']\n",
    "\n",
    "# creating a column email_category with just zeros\n",
    "ac_dataset['email_category'] = '0'\n",
    "\n",
    "# for loop to check the domain name and classify it based on grouping from\n",
    "# marketing team\n",
    "for index, email in ac_dataset[['email']].iterrows():\n",
    "    domain_name = re.findall('@+\\S+[.com|.org]', email[0])[0]\n",
    "    for pattern in ['@', '.com', '.org']:\n",
    "        domain_name = domain_name.replace(pattern, '')\n",
    "    if domain_name in prof_email:\n",
    "        ac_dataset.loc[index, 'email_category'] = 'Professional'\n",
    "    elif domain_name in per_email:\n",
    "        ac_dataset.loc[index, 'email_category'] = 'Personal'\n",
    "    elif domain_name in junk_email:\n",
    "        ac_dataset.loc[index, 'email_category'] = 'Junk'\n",
    "    else:\n",
    "        ac_dataset.loc[index, 'email_category'] = 'Undefined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ac_dataset['email_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at all the other categorical variables\n",
    "\n",
    "# printing columns\n",
    "# print(f\"\"\"\n",
    "# Success of Cross Sell Promotion\n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['cross_sell_success'].value_counts()}\n",
    "\n",
    "\n",
    "# Customer's Residence has Package Room\n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['package_locker'].value_counts()}\n",
    "\n",
    "\n",
    "# Email Categories\n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['email_category'].value_counts()}\n",
    "\n",
    "\n",
    "# Customer's Residence has Package Room has Refrigerator \n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['refrigerated_locker'].value_counts()}\n",
    "\n",
    "\n",
    "# Customer Specified Tastes and Preference\n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['tastes_and_preferences'].value_counts()}\n",
    "\n",
    "\n",
    "# Customer's Registered Number Mobile (1) or Landline (0)\n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['mobile_number'].value_counts()}\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function for categorical boxplots\n",
    "# def categorical_boxplots(response, cat_var, data):\n",
    "#     \"\"\"\n",
    "# \tThis function is designed to generate a boxplot for  can be used for categorical variables.\n",
    "#     Make sure matplotlib.pyplot and seaborn have been imported (as plt and sns).\n",
    "\n",
    "#     PARAMETERS\n",
    "# \t----------\n",
    "# \tresponse : str, response variable\n",
    "# \tcat_var  : str, categorical variable\n",
    "# \tdata     : DataFrame of the response and categorical variables\n",
    "# \t\"\"\"\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize = (8, 8))\n",
    "    \n",
    "#     sns.boxplot(x    = cat_var,\n",
    "#                 y    = response,\n",
    "#                 data = data)\n",
    "    \n",
    "#     plt.suptitle(\"\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calling the function for each categorical variable\n",
    "# categorical_boxplots(response = 'revenue',\n",
    "# \t\t\t\t\t cat_var  = 'cross_sell_success',\n",
    "# \t\t\t\t\t data     = ac_dataset)\n",
    "\n",
    "# categorical_boxplots(response = 'revenue',\n",
    "# \t\t\t\t\t cat_var  = 'package_locker',\n",
    "# \t\t\t\t\t data     = ac_dataset)\n",
    "\n",
    "# categorical_boxplots(response = 'revenue',\n",
    "# \t\t\t\t\t cat_var  = 'email_category',\n",
    "# \t\t\t\t\t data     = ac_dataset)\n",
    "\n",
    "# categorical_boxplots(response = 'revenue',\n",
    "# \t\t\t\t\t cat_var  = 'refrigerated_locker',\n",
    "# \t\t\t\t\t data     = ac_dataset)\n",
    "\n",
    "# categorical_boxplots(response = 'revenue',\n",
    "# \t\t\t\t\t cat_var  = 'tastes_and_preferences',\n",
    "# \t\t\t\t\t data     = ac_dataset)\n",
    "\n",
    "# categorical_boxplots(response = 'revenue',\n",
    "# \t\t\t\t\t cat_var  = 'mobile_number',\n",
    "# \t\t\t\t\t data     = ac_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding email_category\n",
    "one_hot_email = pd.get_dummies(ac_dataset['email_category'], drop_first = True)\n",
    "\n",
    "# dropping categorical variable email_category after it's been encoded\n",
    "ac_dataset = ac_dataset.drop('email_category', axis = 1)\n",
    "\n",
    "# joining codings together\n",
    "ac_dataset = ac_dataset.join(one_hot_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all column names to lowercase\n",
    "ac_dataset.columns = ac_dataset.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking into the count variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset documentation has informed us that a column was mislabeled\n",
    "# the column largest_order_size is meant to be average number of meals ordered\n",
    "\n",
    "# changing the column name\n",
    "ac_dataset = ac_dataset.rename(columns = {'largest_order_size': 'avg_order_size'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if column name was changed successfully \n",
    "# print('largest_order_size' in ac_dataset.columns)\n",
    "\n",
    "# print('avg_order_size'in ac_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating variable for count varibale columns\n",
    "count_variables = ['avg_clicks_per_visit', 'median_meal_rating', \n",
    "                   'avg_order_size', 'pc_logins', 'unique_meals_purch', \n",
    "                   'contacts_w_customer_service', 'product_categories_viewed']\n",
    "\n",
    "# creating variable for count varibale columns with zeros\n",
    "count_variables_zero = ['total_photos_viewed', 'master_classes_attended',\n",
    "                        'mobile_logins', 'weekly_plan', 'early_deliveries',\n",
    "                        'late_deliveries', 'cancellations_before_noon',\n",
    "                        'cancellations_after_noon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scatterplot(response, var, data):\n",
    "#     \"\"\"\n",
    "# \tThis function is designed to generate a scatterplot that can be used for variables.\n",
    "#     Make sure matplotlib.pyplot and seaborn have been imported (as plt and sns).\n",
    "\n",
    "#     PARAMETERS\n",
    "# \t----------\n",
    "# \tresponse : str, response variable\n",
    "# \tvar  : str, variable\n",
    "# \tdata     : DataFrame of the response and categorical variables\n",
    "# \t\"\"\"\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "#     sns.scatterplot(x = data[var],\n",
    "#                     y = data[response],\n",
    "#                     color = 'b')\n",
    "    \n",
    "#     plt.suptitle(\"\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function for count boxplots\n",
    "# def count_boxplots(response, count_var, data):\n",
    "#     \"\"\"\n",
    "# \tThis function is designed to generate a boxplot for  can be used for count variables.\n",
    "#     Make sure matplotlib.pyplot and seaborn have been imported (as plt and sns).\n",
    "\n",
    "#     PARAMETERS\n",
    "# \t----------\n",
    "# \tresponse : str, response variable\n",
    "# \tcat_var  : str, categorical variable\n",
    "# \tdata     : DataFrame of the response and categorical variables\n",
    "# \t\"\"\"\n",
    "    \n",
    "#     fig, ax = plt.subplots(figsize = (12, 8))\n",
    "    \n",
    "#     sns.boxplot(x    = count_var,\n",
    "#                 y    = response,\n",
    "#                 data = data)\n",
    "    \n",
    "#     plt.suptitle(\"\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calling the function for each count varibale column with zeros\n",
    "# for variable in count_variables_zero:\n",
    "#     scatterplot(response = 'revenue', \n",
    "#                 var = variable, \n",
    "#                 data = ac_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy variable for each variable in count_variables_zero\n",
    "\n",
    "# for each variable in count_variables_zero\n",
    "for variable in count_variables_zero:\n",
    "    # create column filled with zero\n",
    "    ac_dataset['has_' + variable] = 0\n",
    "    # creating index and value variables for each row\n",
    "    for index, value in ac_dataset.iterrows():\n",
    "        # if value in variable column greater than zero\n",
    "        if ac_dataset.loc[index, (variable)] > 0:\n",
    "            # turn the value in that index to one\n",
    "            ac_dataset.loc[index, ('has_' + variable)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking results\n",
    "# ac_dataset.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# looking at all the other categorical variables\n",
    "\n",
    "# printing columns\n",
    "# print(f\"\"\"\n",
    "# Average Clicks Per Visit\n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['avg_clicks_per_visit'].value_counts()}\n",
    "\n",
    "\n",
    "# Median Meal Rating\n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['median_meal_rating'].value_counts()}\n",
    "\n",
    "\n",
    "# Average Order Size\n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['avg_order_size'].value_counts()}\n",
    "\n",
    "\n",
    "# PC Logins\n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['pc_logins'].value_counts()}\n",
    "\n",
    "\n",
    "# Unique Meals Purchased\n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['unique_meals_purch'].value_counts()}\n",
    "\n",
    "\n",
    "# Contacts with Customer Service\n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['contacts_w_customer_service'].value_counts()}\n",
    "\n",
    "\n",
    "# Product Categories Viewed\n",
    "# --------------------------------------------------------\n",
    "# {ac_dataset['product_categories_viewed'].value_counts()}\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calling the function for each count variable\n",
    "# for variable in count_variables:\n",
    "#     count_boxplots(response = 'revenue',\n",
    "#                    count_var  = variable,\n",
    "#                    data     = ac_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing a log transform on all count variables \n",
    "for variable in count_variables:\n",
    "    ac_dataset['log_' + variable] = np.log10(ac_dataset[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at total_meals_ordered\n",
    "\n",
    "# setting figure size\n",
    "# fig, ax = plt.subplots(figsize = (10, 8))\n",
    "\n",
    "# developing a scatterplot\n",
    "# sns.scatterplot(x = ac_dataset['total_meals_ordered'],\n",
    "#                 y = ac_dataset['log_revenue'],\n",
    "#                 color = 'r')\n",
    "\n",
    "# adding labels but not adding title\n",
    "# plt.xlabel(xlabel = 'Total Meals Ordered')\n",
    "# plt.ylabel(ylabel = 'Log Revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing log transformation on total_meals_ordered\n",
    "ac_dataset['log_total_meals_ordered'] = np.log10(ac_dataset['total_meals_ordered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting figure size\n",
    "# fig, ax = plt.subplots(figsize = (10, 8))\n",
    "\n",
    "# developing a scatterplot\n",
    "# sns.scatterplot(x = ac_dataset['log_total_meals_ordered'],\n",
    "#                 y = ac_dataset['log_revenue'],\n",
    "#                 color = 'r')\n",
    "\n",
    "# adding labels but not adding title\n",
    "# plt.xlabel(xlabel = 'Log Total Meals Ordered')\n",
    "# plt.ylabel(ylabel = 'Log Revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ac_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_one = ['revenue', 'cross_sell_success', 'email', 'total_meals_ordered', \n",
    "# 'unique_meals_purch', 'contacts_w_customer_service', \n",
    "# 'product_categories_viewed', 'avg_time_per_site_visit', 'mobile_number', \n",
    "# 'cancellations_before_noon']\n",
    "\n",
    "\n",
    "# corr_two = ['cancellations_after_noon', 'tastes_and_preferences', \n",
    "# 'pc_logins', 'mobile_logins', 'weekly_plan', 'early_deliveries',\n",
    "# 'late_deliveries', 'package_locker', 'refrigerated_locker', \n",
    "# 'avg_prep_vid_time', 'avg_order_size','master_classes_attended', \n",
    "# 'median_meal_rating']\n",
    "\n",
    "\n",
    "# corr_three = ['avg_clicks_per_visit', 'total_photos_viewed',\n",
    "# 'personal', 'professional', 'has_total_photos_viewed', \n",
    "# 'has_master_classes_attended', 'has_mobile_logins', \n",
    "# 'has_weekly_plan', 'has_early_deliveries', 'has_late_deliveries', \n",
    "# 'has_cancellations_before_noon', 'has_cancellations_after_noon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating correlation set with some variables \n",
    "# corr_set = ac_dataset[corr_one + corr_two]\n",
    "\n",
    "# instantiating a correlation matrix\n",
    "# df_corr = corr_set.corr().round(2)\n",
    "\n",
    "# setting figure size\n",
    "# fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "# visualizing the correlation matrix\n",
    "# sns.heatmap(df_corr,\n",
    "#             cmap = 'coolwarm',\n",
    "#             square = True,\n",
    "#             annot = True,\n",
    "#             linecolor = 'black',\n",
    "#             linewidths = 0.5)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating correlation set with some variables \n",
    "# corr_set = ac_dataset[corr_one + corr_three]\n",
    "\n",
    "# instantiating a correlation matrix\n",
    "# df_corr = corr_set.corr().round(2)\n",
    "\n",
    "# setting figure size\n",
    "# fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "# visualizing the correlation matrix\n",
    "# sns.heatmap(df_corr,\n",
    "#             cmap = 'coolwarm',\n",
    "#             square = True,\n",
    "#             annot = True,\n",
    "#             linecolor = 'black',\n",
    "#             linewidths = 0.5)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating correlation set with some variables \n",
    "# corr_set = ac_dataset[corr_two + corr_three]\n",
    "\n",
    "# instantiating a correlation matrix\n",
    "# df_corr = corr_set.corr().round(2)\n",
    "\n",
    "# setting figure size\n",
    "# fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "# visualizing the correlation matrix\n",
    "# sns.heatmap(df_corr,\n",
    "#             cmap = 'coolwarm',\n",
    "#             square = True,\n",
    "#             annot = True,\n",
    "#             linecolor = 'black',\n",
    "#             linewidths = 0.5)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OLS REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a copy of housing\n",
    "# ac_dataset_explanatory = ac_dataset.copy()\n",
    "\n",
    "# dropping SalePrice and Order from the explanatory variable set\n",
    "# ac_dataset_explanatory = ac_dataset_explanatory.drop(['revenue',\n",
    "#                                                 'log_revenue'], axis = 1)\n",
    "\n",
    "# formatting each explanatory variable for statsmodels\n",
    "# for val in ac_dataset_explanatory:\n",
    "#     print(f\"{val} +\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a (Pearson) correlation matrix\n",
    "# df_corr = ac_dataset.corr().round(2)\n",
    "\n",
    "# printing (Pearson) correlations with SalePrice\n",
    "# print(df_corr.loc['revenue'].sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing explanatory variable data\n",
    "ac_data = ac_dataset.drop(['revenue', 'log_revenue', 'email'], axis = 1)\n",
    "\n",
    "# preparing response variables\n",
    "ac_target = ac_dataset.loc[ : , 'revenue']\n",
    "log_ac_target = ac_dataset.loc[ : , 'log_revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training and testing sets for revenue\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             ac_data,\n",
    "#             ac_target,\n",
    "#             test_size = 0.25,\n",
    "#             random_state = 219)\n",
    "\n",
    "# merging X_train and y_train so that they can be used in statsmodels\n",
    "# ac_train = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "# build a model\n",
    "# lm_best = smf.ols(formula =  \"\"\"revenue ~ cross_sell_success +\n",
    "#                                         total_meals_ordered +\n",
    "#                                         unique_meals_purch +\n",
    "#                                         contacts_w_customer_service +\n",
    "#                                         product_categories_viewed +\n",
    "#                                         avg_time_per_site_visit +\n",
    "#                                         mobile_number +\n",
    "#                                         cancellations_before_noon +\n",
    "#                                         cancellations_after_noon +\n",
    "#                                         tastes_and_preferences +\n",
    "#                                         pc_logins +\n",
    "#                                         mobile_logins +\n",
    "#                                         weekly_plan +\n",
    "#                                         early_deliveries +\n",
    "#                                         late_deliveries +\n",
    "#                                         package_locker +\n",
    "#                                         refrigerated_locker +\n",
    "#                                         avg_prep_vid_time +\n",
    "#                                         avg_order_size +\n",
    "#                                         master_classes_attended +\n",
    "#                                         median_meal_rating +\n",
    "#                                         avg_clicks_per_visit +\n",
    "#                                         total_photos_viewed +\n",
    "#                                         log_avg_time_per_site_visit +\n",
    "#                                         log_avg_prep_vid_time +\n",
    "#                                         personal +\n",
    "#                                         professional +\n",
    "#                                         has_total_photos_viewed +\n",
    "#                                         has_master_classes_attended +\n",
    "#                                         has_mobile_logins +\n",
    "#                                         has_weekly_plan +\n",
    "#                                         has_early_deliveries +\n",
    "#                                         has_late_deliveries +\n",
    "#                                         has_cancellations_before_noon +\n",
    "#                                         has_cancellations_after_noon +\n",
    "#                                         log_avg_clicks_per_visit +\n",
    "#                                         log_median_meal_rating +\n",
    "#                                         log_avg_order_size +\n",
    "#                                         log_pc_logins +\n",
    "#                                         log_unique_meals_purch +\n",
    "#                                         log_contacts_w_customer_service +\n",
    "#                                         log_product_categories_viewed +\n",
    "#                                         log_total_meals_ordered\"\"\",\n",
    "#                                 data = ac_train)\n",
    "\n",
    "# fit the model based on the data\n",
    "# results = lm_best.fit()\n",
    "\n",
    "# analyze the summary output\n",
    "# print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training and testing sets for log_revenue\n",
    "# log_X_train, log_X_test, log_y_train, log_y_test = train_test_split(\n",
    "#             ac_data,\n",
    "#             log_ac_target,\n",
    "#             test_size = 0.25,\n",
    "#             random_state = 219)\n",
    "\n",
    "# merging X_train and y_train so that they can be used in statsmodels\n",
    "# ac_train = pd.concat([log_X_train, log_y_train], axis = 1)\n",
    "\n",
    "# build a model\n",
    "# lm_best = smf.ols(formula =  \"\"\"log_revenue ~ cross_sell_success +\n",
    "#                                         total_meals_ordered +\n",
    "#                                         unique_meals_purch +\n",
    "#                                         contacts_w_customer_service +\n",
    "#                                         product_categories_viewed +\n",
    "#                                         avg_time_per_site_visit +\n",
    "#                                         mobile_number +\n",
    "#                                         cancellations_before_noon +\n",
    "#                                         cancellations_after_noon +\n",
    "#                                         tastes_and_preferences +\n",
    "#                                         pc_logins +\n",
    "#                                         mobile_logins +\n",
    "#                                         weekly_plan +\n",
    "#                                         early_deliveries +\n",
    "#                                         late_deliveries +\n",
    "#                                         package_locker +\n",
    "#                                         refrigerated_locker +\n",
    "#                                         avg_prep_vid_time +\n",
    "#                                         avg_order_size +\n",
    "#                                         master_classes_attended +\n",
    "#                                         median_meal_rating +\n",
    "#                                         avg_clicks_per_visit +\n",
    "#                                         total_photos_viewed +\n",
    "#                                         log_avg_time_per_site_visit +\n",
    "#                                         log_avg_prep_vid_time +\n",
    "#                                         personal +\n",
    "#                                         professional +\n",
    "#                                         has_total_photos_viewed +\n",
    "#                                         has_master_classes_attended +\n",
    "#                                         has_mobile_logins +\n",
    "#                                         has_weekly_plan +\n",
    "#                                         has_early_deliveries +\n",
    "#                                         has_late_deliveries +\n",
    "#                                         has_cancellations_before_noon +\n",
    "#                                         has_cancellations_after_noon +\n",
    "#                                         log_avg_clicks_per_visit +\n",
    "#                                         log_median_meal_rating +\n",
    "#                                         log_avg_order_size +\n",
    "#                                         log_pc_logins +\n",
    "#                                         log_unique_meals_purch +\n",
    "#                                         log_contacts_w_customer_service +\n",
    "#                                         log_product_categories_viewed +\n",
    "#                                         log_total_meals_ordered\"\"\",\n",
    "#                                 data = ac_train)\n",
    "\n",
    "# fit the model based on the data\n",
    "# results = lm_best.fit()\n",
    "\n",
    "# analyze the summary output\n",
    "# print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training and testing sets for log_revenue removing high p-values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            ac_data,\n",
    "            log_ac_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging X_train and y_train so that they can be used in statsmodels\n",
    "ac_train = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "# build a model\n",
    "lm_best = smf.ols(formula =  \"\"\"log_revenue ~ cross_sell_success +\n",
    "                                        total_meals_ordered +\n",
    "                                        unique_meals_purch +\n",
    "                                        product_categories_viewed +\n",
    "                                        avg_order_size +\n",
    "                                        median_meal_rating +\n",
    "                                        total_photos_viewed +\n",
    "                                        log_avg_prep_vid_time +\n",
    "                                        log_median_meal_rating +\n",
    "                                        log_unique_meals_purch +\n",
    "                                        log_contacts_w_customer_service +\n",
    "                                        log_product_categories_viewed +\n",
    "                                        log_total_meals_ordered\"\"\",\n",
    "                                data = ac_train)\n",
    "\n",
    "# fit the model based on the data\n",
    "results = lm_best.fit()\n",
    "\n",
    "# analyze the summary output\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variables = ['cross_sell_success', 'total_meals_ordered', 'unique_meals_purch',\n",
    "               'product_categories_viewed', 'avg_order_size', 'median_meal_rating',\n",
    "               'total_photos_viewed', 'log_avg_prep_vid_time', \n",
    "               'log_median_meal_rating', 'log_unique_meals_purch',\n",
    "               'log_contacts_w_customer_service', 'log_product_categories_viewed',\n",
    "               'log_total_meals_ordered']\n",
    "\n",
    "# preparing x-variables from the OLS model\n",
    "ols_data = ac_dataset[x_variables]\n",
    "\n",
    "# preparing training and testing sets for log_revenue removing high p-values\n",
    "X_train_ols, X_test_ols, y_train_ols, y_test_ols = train_test_split(\n",
    "                                                        ols_data,\n",
    "                                                        log_ac_target,\n",
    "                                                        test_size = 0.25,\n",
    "                                                        random_state = 219)\n",
    "\n",
    "# instantiating model object\n",
    "lr = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# fitting to training data \n",
    "lr_fit = lr.fit(X_train_ols, y_train_ols)\n",
    "\n",
    "# predicting on new data\n",
    "lr_pred = lr_fit.predict(X_test_ols)\n",
    "\n",
    "# scoring the results\n",
    "print('OLS Training Score :', lr.score(X_train_ols, y_train_ols).round(4))  # using R-square\n",
    "print('OLS Testing Score  :',  lr.score(X_test_ols, y_test_ols).round(4)) # using R-square\n",
    "\n",
    "lr_train_score = lr.score(X_train_ols, y_train_ols).round(4)\n",
    "lr_test_score  = lr.score(X_test_ols, y_test_ols).round(4)\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('OLS Train-Test Gap :', abs(lr_train_score - lr_test_score).round(4))\n",
    "lr_test_gap = abs(lr_train_score - lr_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "lr_model_values = zip(ac_dataset[x_variables].columns,\n",
    "                     lr_fit.coef_.round(decimals = 4))\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "lr_model_lst = [('intercept', lr_fit.intercept_.round(decimals = 4))]\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in lr_model_values:\n",
    "    lr_model_lst.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LASSO REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training and testing sets for log_revenue removing high p-values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            ac_data,\n",
    "            log_ac_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing library for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# using hyperparameter tuning\n",
    "param_grid = {'alpha': np.arange(0.1, 1.1, 0.1)}\n",
    "lasso = sklearn.linear_model.Lasso()\n",
    "lasso_cv = GridSearchCV(lasso, param_grid, cv = 10)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "print(lasso_cv.best_params_)\n",
    "print(lasso_cv.best_score_)\n",
    "\n",
    "# saving best score as a variable\n",
    "lass_best_param = lasso_cv.best_params_['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a model object\n",
    "#lasso = sklearn.linear_model.Lasso(alpha = lass_best_param, normalize = False)\n",
    "lasso = sklearn.linear_model.Lasso(alpha = 0.4, normalize = False)\n",
    "\n",
    "# fitting to training data \n",
    "lasso_fit = lasso.fit(X_train, y_train)\n",
    "\n",
    "# predicting on new data\n",
    "lasso_pred = lasso_fit.predict(X_test)\n",
    "\n",
    "# scoring the results\n",
    "print('Lasso Training Score :', lasso.score(X_train, y_train).round(4))  # using R-square\n",
    "print('Lasso Testing Score  :',  lasso.score(X_test, y_test).round(4)) # using R-square\n",
    "\n",
    "lasso_train_score = lasso.score(X_train, y_train).round(4)\n",
    "lasso_test_score  = lasso.score(X_test, y_test).round(4)\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Lasso Train-Test Gap :', abs(lasso_train_score - lasso_test_score).round(4))\n",
    "lasso_test_gap = abs(lasso_train_score - lasso_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "lasso_model_values = zip(ac_dataset.columns, lasso_fit.coef_.round(decimals = 4))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "lasso_model_lst = [('intercept', lasso_fit.intercept_.round(decimals = 4))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in lasso_model_values:\n",
    "    lasso_model_lst.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping coefficients that are equal to zero\n",
    "for feature, coefficient in lasso_model_lst:  \n",
    "        if coefficient == 0:\n",
    "            lasso_model_lst.remove((feature, coefficient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training and testing sets for log_revenue removing high p-values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            ac_data,\n",
    "            log_ac_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a model object\n",
    "ard = sklearn.linear_model.ARDRegression()\n",
    "\n",
    "# fitting to training data \n",
    "ard_fit = ard.fit(X_train, y_train)\n",
    "\n",
    "# predicting on new data\n",
    "ard_pred = ard_fit.predict(X_test)\n",
    "\n",
    "# scoring the results\n",
    "print('ARD Training Score :', ard.score(X_train, y_train).round(4))  # using R-square\n",
    "print('ARD Testing Score  :',  ard.score(X_test, y_test).round(4)) # using R-square\n",
    "\n",
    "ard_train_score = ard.score(X_train, y_train).round(4)\n",
    "ard_test_score  = ard.score(X_test, y_test).round(4)\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('ARD Train-Test Gap :', abs(ard_train_score - ard_test_score).round(4))\n",
    "ard_test_gap = abs(ard_train_score - ard_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "ard_model_values = zip(ac_dataset.columns, ard_fit.coef_.round(decimals = 4))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "ard_model_lst = [('intercept', ard_fit.intercept_.round(decimals = 4))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in ard_model_values:\n",
    "    ard_model_lst.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping coefficients that are equal to zero\n",
    "for feature, coefficient in ard_model_lst:  \n",
    "        if coefficient == 0:\n",
    "            ard_model_lst.remove((feature, coefficient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing explanatory variable data\n",
    "ac_data_two   = ac_dataset.drop(['revenue', 'log_revenue', 'email', \n",
    "                             'log_avg_time_per_site_visit', \n",
    "                             'log_avg_prep_vid_time', 'log_avg_clicks_per_visit',\n",
    "                             'log_median_meal_rating', 'log_avg_order_size', \n",
    "                             'log_pc_logins', 'log_unique_meals_purch',\n",
    "                             'log_contacts_w_customer_service', \n",
    "                             'log_product_categories_viewed', \n",
    "                             'log_total_meals_ordered'], axis = 1)\n",
    "\n",
    "# preparing response variables\n",
    "log_ac_target = ac_dataset.loc[ : , 'log_revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a StandardScaler() object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# FITTING the scaler with the data\n",
    "scaler.fit(ac_data_two)\n",
    "\n",
    "# TRANSFORMING our data after fit\n",
    "X_scaled = scaler.transform(ac_data_two)\n",
    "\n",
    "# converting scaled data into a DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled)\n",
    "\n",
    "# adding labels to the scaled DataFrame\n",
    "X_scaled_df.columns = ac_data_two.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training and testing sets for log_revenue removing logs\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            ac_data_two,\n",
    "            log_ac_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing library for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# using hyperparameter tuning\n",
    "param_grid = {'n_neighbors': np.arange(1, 50),\n",
    "              'algorithm': ['auto']}\n",
    "knn = KNeighborsRegressor()\n",
    "knn_cv = GridSearchCV(knn, param_grid, cv = 5)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "# saving best parameter as variable\n",
    "opt_neighbors = knn_cv.best_params_['n_neighbors']\n",
    "\n",
    "print(knn_cv.best_params_)\n",
    "print(knn_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a model object\n",
    "# knn = KNeighborsRegressor(algorithm = 'auto',\n",
    "#                           n_neighbors = opt_neighbors)\n",
    "knn = KNeighborsRegressor(algorithm = 'auto',\n",
    "                          n_neighbors = 16)\n",
    "\n",
    "# fitting to training data \n",
    "knn_fit = knn.fit(X_train, y_train)\n",
    "\n",
    "# predicting on new data\n",
    "knn_pred = knn_fit.predict(X_test)\n",
    "\n",
    "# scoring the results\n",
    "print('KNN Training Score :', knn.score(X_train, y_train).round(4))  # using R-square\n",
    "print('KNN Testing Score  :',  knn.score(X_test, y_test).round(4)) # using R-square\n",
    "\n",
    "knn_train_score = knn.score(X_train, y_train).round(4)\n",
    "knn_test_score  = knn.score(X_test, y_test).round(4)\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('KNN Train-Test Gap :', abs(knn_train_score - knn_test_score).round(4))\n",
    "knn_test_gap = abs(knn_train_score - knn_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Type'    : ['OLS (Chosen Model)', 'Lasso', 'ARD', 'KNN'],\n",
    "           \n",
    "    'Training Score' : [lr_train_score, lasso_train_score,\n",
    "                  ard_train_score, knn_train_score],\n",
    "           \n",
    "    'Testing Score'  : [lr_test_score, lasso_test_score,\n",
    "                  ard_test_score, knn_test_score],\n",
    "                    \n",
    "    'Train-Test Gap' : [lr_test_gap, lasso_test_gap,\n",
    "                        ard_test_gap, knn_test_gap],\n",
    "                    \n",
    "    'Model Size' : [len(lr_model_lst), len(lasso_model_lst),\n",
    "                    len(ard_model_lst), 'N/A'],\n",
    "                    \n",
    "    'Model Coeffs and Variables' : [lr_model_lst, lasso_model_lst, ard_model_lst, 'N/A']}\n",
    "\n",
    "\n",
    "# converting model_performance into a DataFrame\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "# calling the model_performance variable\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
