{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to time the program\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.formula.api as smf                \n",
    "from sklearn.metrics import confusion_matrix        \n",
    "from sklearn.metrics import roc_auc_score            \n",
    "from sklearn.neighbors import KNeighborsClassifier   \n",
    "from sklearn.neighbors import KNeighborsRegressor    \n",
    "from sklearn.preprocessing import StandardScaler     \n",
    "from sklearn.tree import DecisionTreeClassifier      \n",
    "from sklearn.tree import export_graphviz            \n",
    "from six import StringIO          \n",
    "from IPython.display import Image                    \n",
    "import pydotplus                                    \n",
    "from sklearn.model_selection import RandomizedSearchCV    \n",
    "from sklearn.metrics import make_scorer              \n",
    "from sklearn.ensemble import RandomForestClassifier    \n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing file path as file\n",
    "file = './Apprentice_Chef_Dataset.xlsx'\n",
    "\n",
    "# creating dataset using the file path\n",
    "ac_dataset = pd.read_excel(io = file)\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all column names to lowercase\n",
    "ac_dataset.columns = ac_dataset.columns.str.lower()\n",
    "\n",
    "# # checking information of all columns\n",
    "# ac_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # viewing first five rows of dataset\n",
    "# ac_dataset.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous variables\n",
    "# -------------------------------\n",
    "# revenue                         - log transform\n",
    "# avg_prep_vid_time               - log transform, compute: total time on site\n",
    "# avg_time_per_site_visit         - log transform\n",
    "\n",
    "# Categorical variables\n",
    "# -------------------------------\n",
    "# email                           - one-hot code, drop original\n",
    "# package_locker                  - make categorical\n",
    "# refrigerated_locker             - make categorical\n",
    "# tastes_and_preferences          - make categorical\n",
    "# mobile_number                   - make categorical\n",
    "\n",
    "# Count variables\n",
    "# -------------------------------\n",
    "# avg_clicks_per_visit            - keep\n",
    "# total_photos_viewed             - keep\n",
    "# master_classes_attended         - yes or no?\n",
    "# median_meal_rating              - keep\n",
    "# largest_order_size              - change name to avg_order_size \n",
    "# pc_logins                       - delete\n",
    "# mobile_logins                   - delete\n",
    "# weekly_plan                     - yes or no?\n",
    "# early_deliveries                - add to late\n",
    "# late_deliveries                 - add to early \n",
    "# cancellations_before_noon       - add to cancellations_after_noon\n",
    "# cancellations_after_noon        - add to cancellations_before_noon\n",
    "# total_meals_ordered             - compute : revenue / total meals ordered\n",
    "# unique_meals_purch              - compute : unique / total meals ordered\n",
    "# contacts_w_customer_service     - keep\n",
    "# product_categories_viewed       - keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking if any columns have missing values \n",
    "# ac_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split_feature(col, df, sep = ' ', new_col_name = 'NUM_OF_NAMES'):\n",
    "    \"\"\"\n",
    "Splits values in a string Series (as part of a DataFrame) and sums the number\n",
    "of resulting items. Automatically appends summed column to original DataFrame.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "col          : column to split\n",
    "df           : DataFrame where column is located\n",
    "sep          : string sequence to split by, default ' '\n",
    "new_col_name : name of new column after summing split, default\n",
    "               'number_of_names'\n",
    "\"\"\"\n",
    "    \n",
    "    df[new_col_name] = 0\n",
    "    \n",
    "    \n",
    "    for index, val in df.iterrows():\n",
    "        df.loc[index, new_col_name] = len(df.loc[index, col].split(sep = ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new column for number of names each customer has\n",
    "text_split_feature(col = 'name', df = ac_dataset, \n",
    "                   new_col_name = 'number_of_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns name, first_name, and family_name\n",
    "ac_dataset = ac_dataset.drop(['name', 'first_name', 'family_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking for any null values\n",
    "# print(ac_dataset.isnull().any().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For continuous variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make revenue normally distributed, we do a log transform\n",
    "\n",
    "# log transformation of revenue and saving as new column\n",
    "ac_dataset['log_revenue'] = np.log10(ac_dataset['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# # setting figure size\n",
    "# fig, ax = plt.subplots(figsize = (15, 10))\n",
    "\n",
    "# # ECDF of revenue\n",
    "# plt.subplot(2, 2, 1)\n",
    "# x = np.sort(ac_dataset['revenue'])\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'revenue')\n",
    "\n",
    "# # ECDF of normally distributed array\n",
    "# x = np.sort(np.random.normal(loc = np.mean(ac_dataset['revenue']), \n",
    "#                              scale = np.std(ac_dataset['revenue']),\n",
    "#                              size = 1000))\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'normal ECDF')\n",
    "\n",
    "# _ = plt.legend(loc = \"lower right\")\n",
    "# _ = plt.xlabel('Revenue')\n",
    "# _ = plt.ylabel('ECDF')\n",
    "\n",
    "# plt.margins(0.02) # Keeps data off plot edges plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # ECDF of log_revenue\n",
    "# plt.subplot(2, 2, 2)\n",
    "# x = np.sort(ac_dataset['log_revenue'])\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'log_revenue')\n",
    "\n",
    "# # ECDF of normally distributed array\n",
    "# x = np.sort(np.random.normal(loc = np.mean(ac_dataset['log_revenue']), \n",
    "#                              scale = np.std(ac_dataset['log_revenue']),\n",
    "#                              size = 1000))\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'normal ECDF')\n",
    "\n",
    "# _ = plt.legend(loc = \"lower right\")\n",
    "# _ = plt.xlabel('Log Revenue')\n",
    "# _ = plt.ylabel('ECDF')\n",
    "\n",
    "# # adding vertical lines to mark trend changes\n",
    "# _ = plt.axvline(x = 2.85)\n",
    "# _ = plt.axvline(x = 3.15)\n",
    "# _ = plt.axvline(x = 3.40)\n",
    "\n",
    "# plt.margins(0.02) # Keeps data off plot edges plt.show()\n",
    "\n",
    "# # cleaning up the layout, saving the figures, and displaying the results\n",
    "# _ = plt.tight_layout()\n",
    "# _ = plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_dataset['log_revenue_one'] = 0\n",
    "ac_dataset['log_revenue_two'] = 0\n",
    "ac_dataset['log_revenue_three'] = 0\n",
    "for index, value in ac_dataset.iterrows():\n",
    "    if ac_dataset.loc[index, 'log_revenue'] >= 3.40:\n",
    "        ac_dataset.loc[index, 'log_revenue_three'] = 1\n",
    "    elif ac_dataset.loc[index, 'log_revenue'] >= 3.15:\n",
    "        ac_dataset.loc[index, 'log_revenue_two'] = 1\n",
    "    elif ac_dataset.loc[index, 'log_revenue'] >= 2.85:\n",
    "        ac_dataset.loc[index, 'log_revenue_one'] = 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make avg_prep_vid_time normally distributed, we do a log transform\n",
    "\n",
    "# log transformation of avg_prep_vid_time and saving as new column\n",
    "ac_dataset['log_avg_prep_vid_time'] = np.log10(ac_dataset['avg_prep_vid_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# # setting figure size\n",
    "# fig, ax = plt.subplots(figsize = (15, 10))\n",
    "\n",
    "# # ECDF of avg_prep_vid_time\n",
    "# plt.subplot(2, 2, 1)\n",
    "# x = np.sort(ac_dataset['avg_prep_vid_time'])\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'avg_prep_vid_time')\n",
    "\n",
    "# # ECDF of normally distributed array\n",
    "# x = np.sort(np.random.normal(loc = np.mean(ac_dataset['avg_prep_vid_time']), \n",
    "#                              scale = np.std(ac_dataset['avg_prep_vid_time']),\n",
    "#                              size = 1000))\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'normal ECDF')\n",
    "\n",
    "# _ = plt.legend(loc = \"lower right\")\n",
    "# _ = plt.xlabel('Avg. Time Video Played (Seconds)')\n",
    "# _ = plt.ylabel('ECDF')\n",
    "\n",
    "\n",
    "\n",
    "# # ECDF of log_avg_prep_vid_time\n",
    "# plt.subplot(2, 2, 2)\n",
    "# x = np.sort(ac_dataset['log_avg_prep_vid_time'])\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'log_avg_prep_vid_time')\n",
    "\n",
    "# # ECDF of normally distributed array\n",
    "# x = np.sort(np.random.normal(loc = np.mean(ac_dataset['log_avg_prep_vid_time']), \n",
    "#                              scale = np.std(ac_dataset['log_avg_prep_vid_time']),\n",
    "#                              size = 1000))\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'normal ECDF')\n",
    "\n",
    "# _ = plt.legend(loc = \"lower right\")\n",
    "# _ = plt.xlabel('Log Avg. Time Video Played (Seconds)')\n",
    "# _ = plt.ylabel('ECDF')\n",
    "\n",
    "# plt.margins(0.02) # Keeps data off plot edges plt.show()\n",
    "\n",
    "# # adding vertical lines to mark trend changes\n",
    "# _ = plt.axvline(x = 1.77)\n",
    "# _ = plt.axvline(x = 2.55)\n",
    "\n",
    "\n",
    "# plt.margins(0.02) # Keeps data off plot edges plt.show()\n",
    "\n",
    "# # cleaning up the layout, saving the figures, and displaying the results\n",
    "# _ = plt.tight_layout()\n",
    "# _ = plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_dataset['vid_prep_one'] = 0\n",
    "ac_dataset['vid_prep_two'] = 0\n",
    "for index, value in ac_dataset.iterrows():\n",
    "    if ac_dataset.loc[index, 'log_avg_prep_vid_time'] >= 2.60:\n",
    "        ac_dataset.loc[index, 'vid_prep_one'] = 1\n",
    "    elif ac_dataset.loc[index, 'log_avg_prep_vid_time'] >= 1.75:\n",
    "        ac_dataset.loc[index, 'vid_prep_two'] = 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make avg_time_per_site_visit normally distributed, we do a log transform\n",
    "\n",
    "# log transformation of avg_time_per_site_visit and saving as new column\n",
    "ac_dataset['log_avg_time_per_site_visit'] = np.log10(ac_dataset['avg_time_per_site_visit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_style": "center",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # setting figure size\n",
    "# fig, ax = plt.subplots(figsize = (15, 10))\n",
    "\n",
    "# # ECDF of avg_time_per_site_visit\n",
    "# plt.subplot(2, 2, 1)\n",
    "# x = np.sort(ac_dataset['avg_time_per_site_visit'])\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'avg_time_per_site_visit')\n",
    "\n",
    "# # ECDF of normally distributed array\n",
    "# x = np.sort(np.random.normal(loc = np.mean(ac_dataset['avg_time_per_site_visit']), \n",
    "#                              scale = np.std(ac_dataset['avg_time_per_site_visit']),\n",
    "#                              size = 1000))\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'normal ECDF')\n",
    "\n",
    "# _ = plt.legend(loc = \"lower right\")\n",
    "# _ = plt.xlabel('Avg. Time Customer Spent per Web/Mobile Visit')\n",
    "# _ = plt.ylabel('ECDF')\n",
    "\n",
    "# plt.margins(0.02) # Keeps data off plot edges plt.show()\n",
    "\n",
    "\n",
    "# # ECDF of log_avg_time_per_site_visit\n",
    "# plt.subplot(2, 2, 2)\n",
    "# x = np.sort(ac_dataset['log_avg_time_per_site_visit'])\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'log_avg_time_per_site_visit')\n",
    "\n",
    "# # ECDF of normally distributed array\n",
    "# x = np.sort(np.random.normal(loc = np.mean(ac_dataset['log_avg_time_per_site_visit']), \n",
    "#                              scale = np.std(ac_dataset['log_avg_time_per_site_visit']),\n",
    "#                              size = 1000))\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'normal ECDF')\n",
    "\n",
    "\n",
    "# # adding vertical lines to mark trend changes\n",
    "# _ = plt.axvline(x = 1.75)\n",
    "# _ = plt.axvline(x = 2.00)\n",
    "# _ = plt.axvline(x = 2.50)\n",
    "\n",
    "# _ = plt.legend(loc = \"lower right\")\n",
    "# _ = plt.xlabel('Log Avg. Time Customer Spent per Web/Mobile Visit')\n",
    "# _ = plt.ylabel('ECDF')\n",
    "\n",
    "# plt.margins(0.02) # Keeps data off plot edges plt.show()\n",
    "\n",
    "# # cleaning up the layout, saving the figures, and displaying the results\n",
    "# _ = plt.tight_layout()\n",
    "# _ = plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_dataset['visit_time_one'] = 0\n",
    "ac_dataset['visit_time_two'] = 0\n",
    "ac_dataset['visit_time_three'] = 0\n",
    "for index, value in ac_dataset.iterrows():\n",
    "    if ac_dataset.loc[index, 'log_avg_prep_vid_time'] >= 2.50:\n",
    "        ac_dataset.loc[index, 'visit_time_three'] = 1\n",
    "    elif ac_dataset.loc[index, 'log_avg_prep_vid_time'] >= 2.00:\n",
    "        ac_dataset.loc[index, 'visit_time_two'] = 1\n",
    "    elif ac_dataset.loc[index, 'log_avg_prep_vid_time'] >= 1.75:\n",
    "        ac_dataset.loc[index, 'visit_time_one'] = 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineering feature for total time spent on website\n",
    "ac_dataset['total_avg_time_on_site'] = ac_dataset['avg_prep_vid_time'] +\\\n",
    "                                   ac_dataset['avg_time_per_site_visit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make total_time_on_site normally distributed, we do a log transform\n",
    "\n",
    "# log transformation of total_avg_time_on_site and saving as new column\n",
    "ac_dataset['log_total_avg_time_on_site'] = np.log10(ac_dataset['total_avg_time_on_site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# # setting figure size\n",
    "# fig, ax = plt.subplots(figsize = (15, 10))\n",
    "\n",
    "# # ECDF of avg_time_per_site_visit\n",
    "# plt.subplot(2, 2, 1)\n",
    "# x = np.sort(ac_dataset['total_avg_time_on_site'])\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'total_avg_time_on_site')\n",
    "\n",
    "# # ECDF of normally distributed array\n",
    "# x = np.sort(np.random.normal(loc = np.mean(ac_dataset['total_avg_time_on_site']), \n",
    "#                              scale = np.std(ac_dataset['total_avg_time_on_site']),\n",
    "#                              size = 1000))\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'normal ECDF')\n",
    "\n",
    "# _ = plt.legend(loc = \"lower right\")\n",
    "# _ = plt.xlabel('Total Avg. Time Customer Spent on Site')\n",
    "# _ = plt.ylabel('ECDF')\n",
    "\n",
    "\n",
    "# # ECDF of log_avg_time_per_site_visit\n",
    "# plt.subplot(2, 2, 2)\n",
    "# x = np.sort(ac_dataset['log_total_avg_time_on_site'])\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'log_total_avg_time_on_site')\n",
    "\n",
    "# # ECDF of normally distributed array\n",
    "# x = np.sort(np.random.normal(loc = np.mean(ac_dataset['log_total_avg_time_on_site']), \n",
    "#                              scale = np.std(ac_dataset['log_total_avg_time_on_site']),\n",
    "#                              size = 1000))\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'normal ECDF')\n",
    "\n",
    "\n",
    "# # adding vertical lines to mark trend changes\n",
    "# _ = plt.axvline(x = 2.80)\n",
    "\n",
    "# _ = plt.legend(loc = \"lower right\")\n",
    "# _ = plt.xlabel('Log Total Avg. Time Customer Spent on Site')\n",
    "# _ = plt.ylabel('ECDF')\n",
    "\n",
    "# plt.margins(0.02) # Keeps data off plot edges plt.show()\n",
    "\n",
    "\n",
    "# # cleaning up the layout, saving the figures, and displaying the results\n",
    "# _ = plt.tight_layout()\n",
    "# _ = plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_dataset['time_on_site_one'] = 0\n",
    "for index, value in ac_dataset.iterrows():\n",
    "    if ac_dataset.loc[index, 'log_avg_prep_vid_time'] >= 2.50:\n",
    "        ac_dataset.loc[index, 'time_on_site_one'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Categorical Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the marketing team is very adamant about email classification so emails need\n",
    "# to be classsified as requested\n",
    "\n",
    "# creating lists for different categories\n",
    "prof_email = ['mmm', 'amex', 'apple', 'boeing', 'caterpillar', 'chevron',\n",
    "             'cisco', 'cocacola', 'disney', 'dupont', 'exxon', 'ge', 'walmart',\n",
    "             'goldmansacs', 'homedepot', 'ibm', 'intel', 'jnj', 'jpmorgan',\n",
    "             'mcdonalds', 'merck', 'microsoft', 'nike', 'pfizer', 'pg',\n",
    "             'travelers', 'unitedtech', 'unitedhealth', 'verizon', 'visa']\n",
    "per_email = ['gmail', 'yahoo', 'protonmail']\n",
    "junk_email = ['me', 'aol', 'hotmail', 'live', 'msn', 'passport']\n",
    "\n",
    "# creating a column email_category with just zeros\n",
    "ac_dataset['email_category'] = '0'\n",
    "\n",
    "# for loop to check the domain name and classify it based on grouping from\n",
    "# marketing team\n",
    "for index, email in ac_dataset[['email']].iterrows():\n",
    "    domain_name = re.findall('@+\\S+[.com|.org]', email[0])[0]\n",
    "    for pattern in ['@', '.com', '.org']:\n",
    "        domain_name = domain_name.replace(pattern, '')\n",
    "    if domain_name in prof_email:\n",
    "        ac_dataset.loc[index, 'email_category'] = 'Professional'\n",
    "    elif domain_name in per_email:\n",
    "        ac_dataset.loc[index, 'email_category'] = 'Personal'\n",
    "    elif domain_name in junk_email:\n",
    "        ac_dataset.loc[index, 'email_category'] = 'Junk'\n",
    "    else:\n",
    "        ac_dataset.loc[index, 'email_category'] = 'Undefined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ac_dataset['email_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding email_category\n",
    "one_hot_email = pd.get_dummies(ac_dataset['email_category'], \n",
    "                               prefix = 'email')\n",
    "\n",
    "# joining codings together\n",
    "ac_dataset = ac_dataset.join(one_hot_email)\n",
    "\n",
    "# changing all column names to lowercase\n",
    "ac_dataset.columns = ac_dataset.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_dataset['domain_name'] = 0\n",
    "\n",
    "for index, email in ac_dataset[['email']].iterrows():\n",
    "    domain_name = re.findall('@+\\S+[.com|.org]', email[0])[0]\n",
    "    for pattern in ['@', '.com', '.org']:\n",
    "        domain_name = domain_name.replace(pattern, '')\n",
    "    ac_dataset.loc[index, 'domain_name'] = domain_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of all email domains highly correlated with cross_sell_success\n",
    "email_list_one = ['aol', 'hotmail', 'msn', 'live']\n",
    "email_list_two = ['microsoft', 'merck', 'jpmorgan', 'pg', 'amex', 'intel', \n",
    "                  'passport', 'me', 'caterpillar', 'unitedhealth']\n",
    "\n",
    "ac_dataset['email_categories'] = 0\n",
    "\n",
    "for index, domain in ac_dataset[['domain_name']].iterrows():\n",
    "    if domain[0] in email_list_one:\n",
    "        ac_dataset.loc[index, 'email_categories'] = 'group_one'\n",
    "    elif domain[0] in email_list_two:\n",
    "        ac_dataset.loc[index, 'email_categories'] = 'group_two'\n",
    "    else:\n",
    "        ac_dataset.loc[index, 'email_categories'] = 'group_three'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding email_category\n",
    "one_hot_email_groups = pd.get_dummies(ac_dataset['email_categories'], \n",
    "                               prefix = 'email')\n",
    "\n",
    "# joining codings together\n",
    "ac_dataset = ac_dataset.join(one_hot_email_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dropping categorical variable email_category & email_Personal after encoding\n",
    "# ac_dataset = ac_dataset.drop(['email_category', 'email_personal', 'email'],\n",
    "#                              axis = 1)\n",
    "\n",
    "\n",
    "ac_dataset = ac_dataset.drop(['email_category', 'email_personal', 'email',\n",
    "                              'email_categories', 'email_group_three', \n",
    "                              'domain_name'],\n",
    "                             axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ac_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Count Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables to split into 0s and 1s\n",
    "variables_to_zero = ['master_classes_attended', 'weekly_plan']\n",
    "\n",
    "# for loop to create column with 0s and 1s\n",
    "for variable in variables_to_zero:\n",
    "    ac_dataset['has_' + variable] = 0\n",
    "    for index, value in ac_dataset.iterrows():\n",
    "        if ac_dataset.loc[index, (variable)] > 0:\n",
    "            ac_dataset.loc[index, ('has_' + variable)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset documentation has informed us that a column was mislabeled\n",
    "# the column largest_order_size is meant to be average number of meals ordered\n",
    "\n",
    "# changing the column name\n",
    "ac_dataset = ac_dataset.rename(columns = {'largest_order_size': 'avg_order_size'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new feature for total logins \n",
    "ac_dataset['total_logins'] = ac_dataset['pc_logins'] + ac_dataset['mobile_logins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if early_delivries and late_deliveries both equal to zero, then customer\n",
    "# got deliveries on-time every time \n",
    "\n",
    "# creating empty column\n",
    "ac_dataset['delivery_time'] = 0\n",
    "\n",
    "# for loop to create column with delivery description\n",
    "for index, value in ac_dataset.iterrows():\n",
    "        #if early and late equals to zero\n",
    "        if ac_dataset.loc[index, 'early_deliveries'] == 0\\\n",
    "        and ac_dataset.loc[index, 'late_deliveries'] == 0:\n",
    "            ac_dataset.loc[index, 'delivery_time'] = 'on_time'\n",
    "        #if early and late greater than zero\n",
    "        elif ac_dataset.loc[index, 'early_deliveries'] > 0\\\n",
    "        and ac_dataset.loc[index, 'late_deliveries'] > 0: \n",
    "            ac_dataset.loc[index, 'delivery_time'] = 'mixed'\n",
    "        #if early greater than zero\n",
    "        elif ac_dataset.loc[index, 'early_deliveries'] > 0: \n",
    "            ac_dataset.loc[index, 'delivery_time'] = 'early' \n",
    "        #if late greater than zero\n",
    "        elif ac_dataset.loc[index, 'late_deliveries'] > 0: \n",
    "            ac_dataset.loc[index, 'delivery_time'] = 'late'\n",
    "        # for handling errors\n",
    "        else:\n",
    "            ac_dataset.loc[index, 'delivery_time'] = 'unknown' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding delivery_time\n",
    "one_hot_email = pd.get_dummies(ac_dataset['delivery_time'], \n",
    "                               prefix = 'delivery_time')\n",
    "\n",
    "# joining codings togetherz\n",
    "ac_dataset = ac_dataset.join(one_hot_email)\n",
    "\n",
    "# changing all column names to lowercase\n",
    "ac_dataset.columns = ac_dataset.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping categorical variables after encoding\n",
    "ac_dataset = ac_dataset.drop(['delivery_time', 'delivery_time_on_time'],\n",
    "                             axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cancellations_before_noon and cancellations_after_noon both equal \n",
    "# to zero, then customer never cancelled \n",
    "\n",
    "# creating empty column\n",
    "ac_dataset['cancellations'] = 0\n",
    "\n",
    "# for loop to create column with cancellation description\n",
    "for index, value in ac_dataset.iterrows():\n",
    "        #if before_noon and after_noon equals to zero\n",
    "        if ac_dataset.loc[index, 'cancellations_before_noon'] == 0\\\n",
    "        and ac_dataset.loc[index, 'cancellations_after_noon'] == 0:\n",
    "            ac_dataset.loc[index, 'cancellations'] = 'none'\n",
    "        #if before_noon greater than zero\n",
    "        elif ac_dataset.loc[index, 'cancellations_before_noon'] > 0: \n",
    "            ac_dataset.loc[index, 'cancellations'] = 'before_noon' \n",
    "        #if after_noon greater than zero\n",
    "        elif ac_dataset.loc[index, 'cancellations_after_noon'] > 0: \n",
    "            ac_dataset.loc[index, 'cancellations'] = 'after_noon'\n",
    "        # for handling errors\n",
    "        else:\n",
    "            ac_dataset.loc[index, 'cancellations'] = 'unknown' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding cancellations\n",
    "one_hot_email = pd.get_dummies(ac_dataset['cancellations'], \n",
    "                               prefix = 'any_cancellations')\n",
    "\n",
    "# joining codings together\n",
    "ac_dataset = ac_dataset.join(one_hot_email)\n",
    "\n",
    "# changing all column names to lowercase\n",
    "ac_dataset.columns = ac_dataset.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping categorical variables after encoding\n",
    "ac_dataset = ac_dataset.drop(['cancellations', 'any_cancellations_none'],\n",
    "                             axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total revenue from each customer divided by total meals ordered should\n",
    "# give the revenue per meal\n",
    "\n",
    "# creating empty column\n",
    "ac_dataset['revenue_per_meal'] = ac_dataset['revenue'] / \\\n",
    "                                 ac_dataset['total_meals_ordered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total unique meals purchased by each customer divided by total meals ordered \n",
    "# should give a ratio of unique meals to total orders\n",
    "\n",
    "# creating empty column\n",
    "ac_dataset['unique_meal_ratio'] = ac_dataset['unique_meals_purch'] / \\\n",
    "                                 ac_dataset['total_meals_ordered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make revenue_per_meal normally distributed, we do a log transform\n",
    "\n",
    "# log transformation of revenue_per_meal and saving as new column\n",
    "ac_dataset['log_revenue_per_meal'] = np.log10(ac_dataset['revenue_per_meal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# # setting figure size\n",
    "# fig, ax = plt.subplots(figsize = (15, 10))\n",
    "\n",
    "# # ECDF of revenue_per_meal\n",
    "# plt.subplot(2, 2, 1)\n",
    "# x = np.sort(ac_dataset['revenue_per_meal'])\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'revenue_per_meal')\n",
    "\n",
    "# # ECDF of normally distributed array\n",
    "# x = np.sort(np.random.normal(loc = np.mean(ac_dataset['revenue_per_meal']), \n",
    "#                              scale = np.std(ac_dataset['revenue_per_meal']),\n",
    "#                              size = 1000))\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'normal ECDF')\n",
    "\n",
    "# _ = plt.legend(loc = \"lower right\")\n",
    "# _ = plt.xlabel('Revenue per Meal')\n",
    "# _ = plt.ylabel('ECDF')\n",
    "\n",
    "\n",
    "# # ECDF of log_revenue_per_meal\n",
    "# plt.subplot(2, 2, 2)\n",
    "# x = np.sort(ac_dataset['log_revenue_per_meal'])\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'log_revenue_per_meal')\n",
    "\n",
    "# # ECDF of normally distributed array\n",
    "# x = np.sort(np.random.normal(loc = np.mean(ac_dataset['log_revenue_per_meal']), \n",
    "#                              scale = np.std(ac_dataset['log_revenue_per_meal']),\n",
    "#                              size = 1000))\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'normal ECDF')\n",
    "\n",
    "# _ = plt.legend(loc = \"lower right\")\n",
    "# _ = plt.xlabel('Log Revenue per Meal')\n",
    "# _ = plt.ylabel('ECDF')\n",
    "\n",
    "# plt.margins(0.02) # Keeps data off plot edges plt.show()\n",
    "\n",
    "# # cleaning up the layout, saving the figures, and displaying the results\n",
    "# _ = plt.tight_layout()\n",
    "# _ = plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make revenue_per_meal normally distributed, we do a log transform\n",
    "\n",
    "# log transformation of revenue_per_meal and saving as new column\n",
    "ac_dataset['log_unique_meal_ratio'] = np.log10(ac_dataset['unique_meal_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# # setting figure size\n",
    "# fig, ax = plt.subplots(figsize = (15, 10))\n",
    "\n",
    "# # ECDF of unique_meal_ratio\n",
    "# plt.subplot(2, 2, 1)\n",
    "# x = np.sort(ac_dataset['unique_meal_ratio'])\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'unique_meal_ratio')\n",
    "\n",
    "# # ECDF of normally distributed array\n",
    "# x = np.sort(np.random.normal(loc = np.mean(ac_dataset['unique_meal_ratio']), \n",
    "#                              scale = np.std(ac_dataset['unique_meal_ratio']),\n",
    "#                              size = 1000))\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'normal ECDF')\n",
    "\n",
    "# _ = plt.legend(loc = \"lower right\")\n",
    "# _ = plt.xlabel('Unique Meal Ratio')\n",
    "# _ = plt.ylabel('ECDF')\n",
    "\n",
    "\n",
    "# # ECDF of log_unique_meal_ratio\n",
    "# plt.subplot(2, 2, 2)\n",
    "# x = np.sort(ac_dataset['log_unique_meal_ratio'])\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'log_unique_meal_ratio')\n",
    "\n",
    "# # ECDF of normally distributed array\n",
    "# x = np.sort(np.random.normal(loc = np.mean(ac_dataset['log_unique_meal_ratio']), \n",
    "#                              scale = np.std(ac_dataset['log_unique_meal_ratio']),\n",
    "#                              size = 1000))\n",
    "# y = np.arange(1, len(x) + 1) / len(x)\n",
    "# _ = plt.plot(x, y, linestyle = '-', label = 'normal ECDF')\n",
    "\n",
    "# _ = plt.legend(loc = \"lower right\")\n",
    "# _ = plt.xlabel('Log Unique Meal Ratio')\n",
    "# _ = plt.ylabel('ECDF')\n",
    "\n",
    "# plt.margins(0.02) # Keeps data off plot edges plt.show()\n",
    "\n",
    "# # cleaning up the layout, saving the figures, and displaying the results\n",
    "# _ = plt.tight_layout()\n",
    "# _ = plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ac_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_dataset['procastinator'] = 0 # customers that have junk mail and spend more time on site\n",
    "ac_dataset['working'] = 0 # custoemrs that have pro email and spend less time on site\n",
    "ac_dataset['active_pc_user'] = 0 # more than mean time on site and have low mobile logins\n",
    "ac_dataset['common_user'] = 0 # part of 75% that pays\n",
    "ac_dataset['weekend_fighter'] = 0 # orders less than usual client but pays more\n",
    "\n",
    "for index, value in ac_dataset.iterrows():\n",
    "    \n",
    "    if ac_dataset.loc[index, 'email_junk'] == 1 and \\\n",
    "    ac_dataset.loc[index, 'avg_time_per_site_visit'] >= 99.6:\n",
    "        ac_dataset.loc[index, 'procastinator'] = 1\n",
    "        \n",
    "    if ac_dataset.loc[index, 'email_professional'] == 1 and \\\n",
    "    ac_dataset.loc[index, 'avg_time_per_site_visit'] <= 99.6:\n",
    "        ac_dataset.loc[index, 'working'] = 1\n",
    "        \n",
    "    if ac_dataset.loc[index, 'pc_logins'] > 5 and \\\n",
    "    ac_dataset.loc[index, 'avg_time_per_site_visit'] >= 99.6 and \\\n",
    "    ac_dataset.loc[index, 'mobile_logins']:\n",
    "        ac_dataset.loc[index, 'active_pc_user'] = 1\n",
    "    \n",
    "    if ac_dataset.loc[index, 'revenue_per_meal'] >= 25:\n",
    "        ac_dataset.loc[index, 'common_user'] = 1\n",
    "        \n",
    "    if ac_dataset.loc[index, 'total_meals_ordered'] <= 60 and \\\n",
    "    ac_dataset.loc[index, 'revenue_per_meal'] > 34:\n",
    "        ac_dataset.loc[index, 'weekend_fighter'] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all unused continuous features\n",
    "ac_dataset = ac_dataset.drop(['revenue', \n",
    "                              'unique_meal_ratio',\n",
    "                              'revenue_per_meal', \n",
    "                              'avg_prep_vid_time', \n",
    "                              'avg_time_per_site_visit',\n",
    "                              'total_avg_time_on_site'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating variable for count varibale columns\n",
    "count_variables = ['avg_clicks_per_visit', 'median_meal_rating', \n",
    "                   'pc_logins', 'unique_meals_purch', \n",
    "                   'contacts_w_customer_service', 'product_categories_viewed']\n",
    "\n",
    "\n",
    "# performing a log transform on all count variables \n",
    "for variable in count_variables:\n",
    "    ac_dataset['log_' + variable] = np.log10(ac_dataset[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all unused continuous features\n",
    "ac_dataset = ac_dataset.drop(count_variables, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables to split into 0s and 1s\n",
    "variables_to_zero = ['cancellations_before_noon', 'cancellations_after_noon',\n",
    "                     'early_deliveries', 'late_deliveries']\n",
    "\n",
    "# for loop to create column with 0s and 1s\n",
    "for variable in variables_to_zero:\n",
    "    ac_dataset['has_' + variable] = 0\n",
    "    for index, value in ac_dataset.iterrows():\n",
    "        if ac_dataset.loc[index, (variable)] > 0:\n",
    "            ac_dataset.loc[index, ('has_' + variable)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ac_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corr = ac_dataset.corr(method = 'pearson').round(2)\n",
    "\n",
    "# df_corr['cross_sell_success'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # correlation heatmap\n",
    "\n",
    "# # setting figure size\n",
    "# fig, ax = plt.subplots(figsize=(50, 50))\n",
    "\n",
    "# # visualizing the correlation matrix\n",
    "# sns.heatmap(df_corr,\n",
    "#             cmap = 'coolwarm',\n",
    "#             square = True,\n",
    "#             annot = True,\n",
    "#             linecolor = 'black',\n",
    "#             linewidths = 0.5,\n",
    "#             cbar = False)\n",
    "\n",
    "# # saving and displaying the correlation matrix\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # declaring explanatory variables\n",
    "# ac_data = ac_dataset.drop('cross_sell_success', axis = 1)\n",
    "\n",
    "# # declaring response variable\n",
    "# ac_target = ac_dataset.loc[ : , 'cross_sell_success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train-test split with stratification\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             ac_data,\n",
    "#             ac_target,\n",
    "#             test_size    = 0.25,\n",
    "#             random_state = 219,\n",
    "#             stratify     = ac_target)\n",
    "\n",
    "# # merging training data for statsmodels\n",
    "# ac_train = pd.concat([X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val in ac_data:\n",
    "#     print(f\" {val} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiating a logistic regression model object\n",
    "# logistic_full = smf.logit(formula = \"\"\"  cross_sell_success ~ \n",
    "#                                                  email_group_one +\n",
    "#                                                  email_professional +\n",
    "#                                                  number_of_names +\n",
    "#                                                  any_cancellations_before_noon +\n",
    "#                                                  cancellations_after_noon +\n",
    "#                                                  procastinator\n",
    "#                                                  \"\"\",\n",
    "#                                          data    = ac_train)\n",
    "\n",
    "# # fitting the model object\n",
    "# results_full = logistic_full.fit()\n",
    "\n",
    "# # checking the results SUMMARY\n",
    "# results_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary to store candidate models\n",
    "\n",
    "candidate_dict = { \n",
    "\n",
    " # full model (set 1)\n",
    " 'logit_full'   : ['email_professional', 'cancellations_before_noon',\n",
    "                   'has_cancellations_before_noon', 'mobile_number',\n",
    "                   'tastes_and_preferences', 'refrigerated_locker',\n",
    "                   'log_pc_logins', 'log_revenue_two', 'has_master_classes_attended',\n",
    "                   'log_contacts_w_customer_service', 'package_locker',\n",
    "                   'has_cancellations_after_noon', 'mobile_logins',\n",
    "                   'log_revenue_one', 'visit_time_one', 'log_avg_clicks_per_visit',\n",
    "                   'log_revenue_per_meal', 'has_weekly_plan', 'has_early_deliveries',\n",
    "                   'weekly_plan', 'log_revenue_three', 'log_unique_meal_ratio',\n",
    "                   'avg_order_size', 'log_avg_time_per_site_visit',\n",
    "                   'visit_time_three', 'vid_prep_two', 'log_median_meal_rating',\n",
    "                   'log_unique_meals_purch', 'vid_prep_one', 'has_late_deliveries',\n",
    "                   'total_logins', 'total_photos_viewed', 'total_meals_ordered',\n",
    "                   'log_product_categories_viewed', 'late_deliveries',\n",
    "                   'early_deliveries', 'visit_time_two', 'log_total_avg_time_on_site',\n",
    "                   'procastinator', 'working', 'active_pc_user', 'common_user', \n",
    "                   'weekend_fighter', 'email_group_one', 'number_of_names'], \n",
    "\n",
    " # full model (set 2)                  \n",
    " 'logit_full_2'   : ['email_professional', 'cancellations_before_noon',\n",
    "                   'has_cancellations_before_noon', 'mobile_number',\n",
    "                   'tastes_and_preferences', 'refrigerated_locker',\n",
    "                   'log_pc_logins', 'log_revenue_two', 'master_classes_attended',                   \n",
    "                   'log_contacts_w_customer_service', 'package_locker',\n",
    "                   'cancellations_after_noon', 'email_junk', 'mobile_logins', \n",
    "                   'log_revenue_one', 'visit_time_one', 'log_avg_clicks_per_visit',\n",
    "                   'has_weekly_plan', 'has_early_deliveries',  \n",
    "                   'weekly_plan', 'log_revenue', 'log_unique_meal_ratio',\n",
    "                   'total_logins', 'log_avg_prep_vid_time',  \n",
    "                   'time_on_site_one', 'vid_prep_two', 'log_median_meal_rating',\n",
    "                   'log_unique_meals_purch', 'vid_prep_one', 'has_late_deliveries',\n",
    "                   'total_logins', 'total_photos_viewed', 'total_meals_ordered',\n",
    "                   'log_product_categories_viewed', 'late_deliveries',\n",
    "                   'early_deliveries', 'visit_time_two', 'log_total_avg_time_on_site',\n",
    "                   'procastinator', 'working', 'active_pc_user', 'common_user', \n",
    "                   'weekend_fighter', 'email_group_one', 'number_of_names'],\n",
    "                      \n",
    " # significant variables only (set 1)\n",
    " 'logit_sig_1'    : ['total_meals_ordered' , 'log_contacts_w_customer_service',\n",
    "                   'mobile_number', 'tastes_and_preferences', 'email_junk',\n",
    "                   'has_master_classes_attended', 'any_cancellations_before_noon',\n",
    "                   'log_revenue_per_meal', 'email_professional', \n",
    "                   'refrigerated_locker'],\n",
    "    \n",
    " # significant variables only (set 2)\n",
    " 'logit_sig_2'  : ['total_meals_ordered', 'log_unique_meals_purch', 'email_junk',\n",
    "                   'email_professional', 'log_unique_meal_ratio', 'mobile_number',\n",
    "                   'log_contacts_w_customer_service', 'tastes_and_preferences',\n",
    "                   'refrigerated_locker', 'log_revenue', \n",
    "                   'log_total_avg_time_on_site', 'has_master_classes_attended',\n",
    "                   'any_cancellations_before_noon', 'number_of_names'],\n",
    "    \n",
    " # significant variables only (set 3)\n",
    " 'logit_sig_3'  : ['total_meals_ordered', 'mobile_number', 'refrigerated_locker',\n",
    "                   'log_contacts_w_customer_service', 'mobile_logins', 'vid_prep_two',\n",
    "                   'has_master_classes_attended', 'tastes_and_preferences',\n",
    "                   'time_on_site_one', 'email_junk', 'email_professional',\n",
    "                   'log_total_avg_time_on_site', 'has_master_classes_attended',\n",
    "                   'total_logins', 'any_cancellations_before_noon',\n",
    "                   'log_revenue_per_meal'],  \n",
    "    \n",
    " # significant variables only (set 4)   \n",
    " 'logit_sig_4'  : ['email_junk', 'email_professional', 'any_cancellations_before_noon',\n",
    "                   'log_total_avg_time_on_site', 'log_avg_time_per_site_visit',\n",
    "                   'log_revenue_per_meal', 'log_unique_meal_ratio', 'log_revenue',\n",
    "                   'total_meals_ordered'],\n",
    "    \n",
    " # significant variables only (set 5)   \n",
    " 'logit_sig_5'  : ['total_meals_ordered', 'mobile_number', 'tastes_and_preferences',\n",
    "                   'refrigerated_locker', 'log_revenue_one', 'vid_prep_two',\n",
    "                   'visit_time_three', 'email_junk', 'email_professional',\n",
    "                   'total_logins', 'any_cancellations_before_noon',\n",
    "                   'log_revenue_per_meal', 'log_pc_logins',\n",
    "                   'log_contacts_w_customer_service'],  \n",
    "\n",
    " # significant variables only (set 6)   \n",
    " 'logit_sig_6'  : ['total_meals_ordered', 'log_contacts_w_customer_service',\n",
    "                   'mobile_number', 'early_deliveries', 'cancellations_before_noon',\n",
    "                   'tastes_and_preferences', 'mobile_logins', 'log_avg_prep_vid_time',\n",
    "                   'log_pc_logins', 'has_early_deliveries', 'email_junk',\n",
    "                   'email_professional', 'log_product_categories_viewed',\n",
    "                   'log_avg_time_per_site_visit'],\n",
    "\n",
    " # significant variables only (set 7)   \n",
    " 'logit_sig_7'  : ['email_junk', 'email_professional', 'mobile_number',\n",
    "                   'early_deliveries', 'log_pc_logins', 'refrigerated_locker',\n",
    "                   'cancellations_before_noon', 'tastes_and_preferences',\n",
    "                   'has_master_classes_attended'],\n",
    "  \n",
    " # significant variables only (set 8)    \n",
    " 'logit_sig_8'  : ['log_contacts_w_customer_service', 'mobile_number',  \n",
    "                   'cancellations_before_noon', 'email_professional',\n",
    "                   'tastes_and_preferences', 'mobile_logins', 'weekly_plan',\n",
    "                   'log_avg_prep_vid_time', 'email_junk', 'log_product_categories_viewed',\n",
    "                   'has_master_classes_attended', 'log_pc_logins', 'procastinator',\n",
    "                   'working', 'active_pc_user', 'common_user', 'weekend_fighter'],\n",
    "    \n",
    " # significant variables only (set 9)    \n",
    " 'logit_sig_9'  : ['log_contacts_w_customer_service', 'mobile_number',  \n",
    "                   'cancellations_before_noon', 'email_professional',\n",
    "                   'tastes_and_preferences', 'mobile_logins', 'weekly_plan',\n",
    "                   'log_avg_prep_vid_time', 'email_group_one', 'log_product_categories_viewed',\n",
    "                   'has_master_classes_attended', 'log_pc_logins', 'procastinator',\n",
    "                   'working', 'active_pc_user', 'common_user', 'weekend_fighter', \n",
    "                   'number_of_names'],\n",
    "    \n",
    " # significant variables only (set 10)   \n",
    " 'logit_sig_10'  : ['total_meals_ordered', 'log_contacts_w_customer_service',\n",
    "                   'mobile_number', 'cancellations_before_noon',\n",
    "                   'tastes_and_preferences', 'mobile_logins', 'weekly_plan',\n",
    "                   'log_avg_prep_vid_time', 'has_master_classes_attended',\n",
    "                   'log_pc_logins', 'email_junk', 'procastinator',\n",
    "                   'email_professional', 'log_product_categories_viewed',\n",
    "                   'working', 'active_pc_user', 'common_user',\n",
    "                   'weekend_fighter', 'number_of_names']\n",
    "\n",
    "}                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "ac_data   =  ac_dataset.loc[ : , candidate_dict['logit_full_2']]\n",
    "ac_target =  ac_dataset.loc[ : , 'cross_sell_success']\n",
    "\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            ac_data,\n",
    "            ac_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = ac_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ########################################\n",
    "# # RandomizedSearchCV\n",
    "# ########################################\n",
    "\n",
    "# # declaring a hyperparameter space\n",
    "# C_space          = np.arange(3.0, 10.0, 0.5)\n",
    "# warm_start_space = [True, False]\n",
    "# solver_space     = ['newton-cg', 'sag', 'lbfgs']\n",
    "# tol_space        = np.arange(0.001, 0.01, 0.02)\n",
    "# multi_class_space = ['auto', 'ovr', 'multinomial']\n",
    "\n",
    "\n",
    "# # creating a hyperparameter grid\n",
    "# param_grid = {'C'            : C_space,\n",
    "#               'warm_start'   : warm_start_space,\n",
    "#               'solver'       : solver_space,\n",
    "#               'tol'          : tol_space,\n",
    "#               'multi_class'  : multi_class_space}\n",
    "\n",
    "# # INSTANTIATING the model object without hyperparameters\n",
    "# lr_tuned = LogisticRegression(random_state = 219,\n",
    "#                               max_iter     = 8000)\n",
    "\n",
    "# # GridSearchCV object\n",
    "# lr_tuned_cv = RandomizedSearchCV(estimator           = lr_tuned,   # the model object\n",
    "#                                  param_distributions = param_grid, # parameters to tune\n",
    "#                                  cv                  = 3,          # how many folds in cross-validation\n",
    "#                                  n_iter              = 250,        # number of combinations of hyperparameters to try\n",
    "#                                  random_state        = 219,        # starting point for random sequence\n",
    "#                                  scoring = make_scorer(\n",
    "#                                            roc_auc_score,\n",
    "#                                            needs_threshold = False)) # scoring criteria (AUC)\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# lr_tuned_cv.fit(ac_data, ac_target)\n",
    "\n",
    "# # PREDICT step is not needed\n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", lr_tuned_cv.best_params_)\n",
    "# print(\"Tuned CV AUC      :\", lr_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg Training ACCURACY: 0.7539\n",
      "LogReg Testing  ACCURACY: 0.7577\n",
      "LogReg Train-Test Gap   : 0.0038\n",
      "\n",
      "True Negatives : 71\n",
      "False Positives: 85\n",
      "False Negatives: 33\n",
      "True Positives : 298\n",
      "\n",
      "AUC Score: 0.6777\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(C = 4.5, max_iter = 8000, random_state = 219,\n",
    "                            tol = 0.001, warm_start = True)\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(ac_data, ac_target)\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(X_test)\n",
    "\n",
    "# SCORING the results\n",
    "print('LogReg Training ACCURACY:', logreg_fit.score(X_train, y_train).round(4))\n",
    "print('LogReg Testing  ACCURACY:', logreg_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(X_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('LogReg Train-Test Gap   :', abs(logreg_train_score - logreg_test_score).round(4))\n",
    "logreg_test_gap = abs(logreg_train_score - logreg_test_score).round(4)\n",
    "\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {logreg_tn}\n",
    "False Positives: {logreg_fp}\n",
    "False Negatives: {logreg_fn}\n",
    "True Positives : {logreg_tp}\n",
    "\"\"\")\n",
    "\n",
    "# area under the roc curve (auc)\n",
    "print(f'''AUC Score: {roc_auc_score(y_true  = y_test,\n",
    "                    y_score = logreg_pred).round(decimals = 4)}''')\n",
    "\n",
    "# saving AUC score for future use\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = logreg_pred).round(decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full One 66%\n",
    "# LogisticRegression(C=8.0, max_iter=8000, multi_class='multinomial',\n",
    "#                    random_state=219, tol=0.001, warm_start=True)\n",
    "\n",
    "# Full Two 67.77%\n",
    "# LogisticRegression(C=4.5, max_iter=8000, random_state=219, tol=0.001,\n",
    "#                   warm_start=True)\n",
    "\n",
    "# Sig 1 62.49%\n",
    "# LogisticRegression(C=3.5, max_iter=8000, random_state=219, solver='newton-cg',\n",
    "#                    tol=0.001, warm_start=True)\n",
    "\n",
    "# Sig 2 66.79%\n",
    "# LogisticRegression(C=4.0, max_iter=8000, random_state=219, tol=0.001,\n",
    "#                    warm_start=True)\n",
    "\n",
    "# Sig 3 63.93%\n",
    "# LogisticRegression(C=6.5, max_iter=8000, random_state=219, tol=0.001,\n",
    "#                    warm_start=True)\n",
    "\n",
    "# Sig 4 60.79%\n",
    "# LogisticRegression(C=5.5, max_iter=8000, random_state=219, tol=0.001,\n",
    "#                    warm_start=True)\n",
    "\n",
    "# Sig 5 62.66%\n",
    "# LogisticRegression(C=8.5, max_iter=8000, random_state=219, tol=0.001,\n",
    "#                   warm_start=True)\n",
    "\n",
    "# Sig 6 62.51%\n",
    "# LogisticRegression(C=4.5, max_iter=8000, random_state=219, tol=0.001,\n",
    "#                    warm_start=True)\n",
    "\n",
    "# Sig 7 62.78%\n",
    "# LogisticRegression(C=4.5, max_iter=8000, random_state=219, tol=0.001,\n",
    "#                    warm_start=True)\n",
    "\n",
    "# Sig 8 62%\n",
    "# LogisticRegression(C=7.5, max_iter=8000, multi_class='multinomial',\n",
    "#                    random_state=219, solver='newton-cg', tol=0.001,\n",
    "#                    warm_start=True)\n",
    "\n",
    "# Sig 9 63%\n",
    "# LogisticRegression(C=3.5, max_iter=8000, random_state=219, solver='newton-cg',\n",
    "#                    tol=0.001, warm_start=True)\n",
    "\n",
    "# Sig 10 66.13%\n",
    "# LogisticRegression(C=8.5, max_iter=8000, multi_class='multinomial',\n",
    "#                    random_state=219, solver='newton-cg', tol=0.001,\n",
    "#                    warm_start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "ac_data   =  ac_dataset.loc[ : , candidate_dict['logit_full_2']]\n",
    "ac_target =  ac_dataset.loc[ : , 'cross_sell_success']\n",
    "\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            ac_data,\n",
    "            ac_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = ac_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # declaring a hyperparameter space\n",
    "# criterion_space = ['gini', 'entropy']\n",
    "# splitter_space  = ['best', 'random']\n",
    "# depth_space     = np.arange(1, 9, 1)\n",
    "# sample_split_space = np.arange(10, 30, 1)\n",
    "# leaf_space      = np.arange(20, 50, 1)\n",
    "# feature_space   = ['auto', 'sqrt', 'log2', None]\n",
    "# split_space     = np.arange(1, 50, 1)\n",
    "# leaf_node_space = np.arange(5, 50, 1)\n",
    "\n",
    "\n",
    "\n",
    "# # creating a hyperparameter grid\n",
    "# param_grid = {'criterion'        : criterion_space,\n",
    "#               'splitter'         : splitter_space,\n",
    "#               'max_depth'        : depth_space,\n",
    "#               'min_samples_leaf' : leaf_space,\n",
    "#               'min_samples_split' : split_space,\n",
    "#               'max_features'     : feature_space,\n",
    "#               'min_samples_split' : sample_split_space,\n",
    "#               'max_leaf_nodes': leaf_node_space}\n",
    "\n",
    "# # INSTANTIATING the model object without hyperparameters\n",
    "# tuned_tree = DecisionTreeClassifier(random_state = 219)\n",
    "\n",
    "# # RandomizedSearchCV object\n",
    "# tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree,\n",
    "#                                    param_distributions   = param_grid,\n",
    "#                                    cv                    = 3,\n",
    "#                                    n_iter                = 1000,\n",
    "#                                    random_state          = 219,\n",
    "#                                    scoring = make_scorer(roc_auc_score,\n",
    "#                                              needs_threshold = False))\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# tuned_tree_cv.fit(ac_data, ac_target)\n",
    "\n",
    "# # PREDICT step is not needed\n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", tuned_tree_cv.best_params_)\n",
    "# print(\"Tuned Training AUC:\", tuned_tree_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7402\n",
      "Testing  ACCURACY: 0.7762\n",
      "LogReg Train-Test Gap   : 0.036\n",
      "\n",
      "True Negatives : 95\n",
      "False Positives: 61\n",
      "False Negatives: 48\n",
      "True Positives : 283\n",
      "\n",
      "AUC Score        : 0.732\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "tree_tuned = DecisionTreeClassifier(max_depth=3, max_leaf_nodes=28, min_samples_leaf=22,\n",
    "                       min_samples_split=19, random_state=219)\n",
    "\n",
    "# FIT step is not needed\n",
    "tree_tuned_fit = tree_tuned.fit(ac_data, ac_target)\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "tree_tuned_pred = tree_tuned_fit.predict(X_test)\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', tree_tuned.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(X_test, y_test).round(4))\n",
    "\n",
    "# saving scoring data for future use\n",
    "tree_tuned_train_score = tree_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "tree_tuned_test_score  = tree_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('LogReg Train-Test Gap   :', abs(tree_tuned_train_score - \\\n",
    "                                       tree_tuned_test_score).round(4))\n",
    "\n",
    "tree_tuned_test_gap = abs(tree_tuned_train_score - tree_tuned_test_score).round(4)\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "tree_tuned_tn, \\\n",
    "tree_tuned_fp, \\\n",
    "tree_tuned_fn, \\\n",
    "tree_tuned_tp = confusion_matrix(y_true = y_test, y_pred = tree_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {tree_tuned_tn}\n",
    "False Positives: {tree_tuned_fp}\n",
    "False Negatives: {tree_tuned_fn}\n",
    "True Positives : {tree_tuned_tp}\n",
    "\"\"\")\n",
    "\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                  y_score = tree_tuned_pred).round(4))\n",
    "\n",
    "# saving the AUC score\n",
    "tree_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = tree_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full One 71.22%\n",
    "# DecisionTreeClassifier(criterion='entropy', max_depth=5, max_leaf_nodes=5,\n",
    "#                        min_samples_leaf=43, min_samples_split=26,\n",
    "#                        random_state=219\n",
    "\n",
    "# Full Two 73.20%\n",
    "# DecisionTreeClassifier(max_depth=3, max_leaf_nodes=28, min_samples_leaf=22,\n",
    "#                        min_samples_split=19, random_state=219)\n",
    "\n",
    "# Sig 1 64.30%\n",
    "# DecisionTreeClassifier(max_depth=5, max_leaf_nodes=10, min_samples_leaf=21,\n",
    "#                        min_samples_split=10, random_state=219,\n",
    "#                        splitter='random')\n",
    "\n",
    "# Sig 2 62.87%\n",
    "# DecisionTreeClassifier(criterion='entropy', max_depth=5, max_leaf_nodes=5,\n",
    "#                        min_samples_leaf=43, min_samples_split=26,\n",
    "#                        random_state=219)\n",
    "\n",
    "# Sig 3 64.30%\n",
    "# DecisionTreeClassifier(max_depth=5, max_leaf_nodes=10, min_samples_leaf=21,\n",
    "#                        min_samples_split=10, random_state=219,\n",
    "#                        splitter='random')\n",
    "\n",
    "# Sig 4 62.09%\n",
    "# DecisionTreeClassifier(criterion='entropy', max_depth=5, max_leaf_nodes=5,\n",
    "#                        min_samples_leaf=43, min_samples_split=26,\n",
    "#                        random_state=219)\n",
    "\n",
    "# Sig 5 64.30%\n",
    "# DecisionTreeClassifier(max_depth=4, max_leaf_nodes=38, min_samples_leaf=21,\n",
    "#                        min_samples_split=10, random_state=219,\n",
    "#                        splitter='random')\n",
    "\n",
    "# Sig 6 62.09%\n",
    "# DecisionTreeClassifier(criterion='entropy', max_depth=1, max_leaf_nodes=46,\n",
    "#                        min_samples_leaf=30, min_samples_split=12,\n",
    "#                        random_state=219, splitter='random')\n",
    "\n",
    "# Sig 7 61.86%\n",
    "# DecisionTreeClassifier(max_depth=5, max_leaf_nodes=10, min_samples_leaf=21,\n",
    "#                        min_samples_split=10, random_state=219,\n",
    "#                        splitter='random')\n",
    "\n",
    "# Sig 8 64.85%\n",
    "# DecisionTreeClassifier(max_depth=4, max_leaf_nodes=34, min_samples_leaf=21,\n",
    "#                        min_samples_split=13, random_state=219)\n",
    "\n",
    "# Sig 9 71.41%\n",
    "# DecisionTreeClassifier(max_depth=3, max_leaf_nodes=48, min_samples_leaf=41,\n",
    "#                        min_samples_split=28, random_state=219)\n",
    "\n",
    "# Sig 10 73.20%\n",
    "# DecisionTreeClassifier(max_depth=3, max_leaf_nodes=28, min_samples_leaf=22,\n",
    "#                        min_samples_split=19, random_state=219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "ac_data   =  ac_dataset.loc[ : , candidate_dict['logit_sig_7']]\n",
    "ac_target =  ac_dataset.loc[ : , 'cross_sell_success']\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            ac_data,\n",
    "            ac_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = ac_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# FITTING the data\n",
    "scaler.fit(ac_data)\n",
    "\n",
    "# TRANSFORMING the data\n",
    "X_scaled     = scaler.transform(ac_data)\n",
    "\n",
    "# converting to a DataFrame\n",
    "X_scaled_df  = pd.DataFrame(X_scaled) \n",
    "\n",
    "# train-test split with the scaled data\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
    "            X_scaled_df,\n",
    "            ac_target,\n",
    "            random_state = 219,\n",
    "            test_size = 0.25,\n",
    "            stratify = ac_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # declaring a hyperparameter space\n",
    "# neighbor_space  = np.arange(1, 20, 1)\n",
    "# weight_space  = ['uniform', 'distance']\n",
    "# algoritm_space       = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "# leaf_space  = np.arange(20, 80, 5)\n",
    "# p_space = np.arange(1, 10, 1)\n",
    "\n",
    "\n",
    "# # creating a hyperparameter grid\n",
    "# param_grid = {'n_neighbors'     : neighbor_space,\n",
    "#               'weights' : weight_space,\n",
    "#               'algorithm'        : algoritm_space,\n",
    "#               'leaf_size'        : leaf_space,\n",
    "#               'p'       : p_space}\n",
    "\n",
    "# # INSTANTIATING a KNN classification model with optimal neighbors\n",
    "# knn_opt = KNeighborsClassifier()\n",
    "\n",
    "# # GridSearchCV object\n",
    "# knn_cv = RandomizedSearchCV(estimator           = knn_opt,\n",
    "#                                param_distributions = param_grid,\n",
    "#                                cv         = 3,\n",
    "#                                n_iter     = 1000,\n",
    "#                                scoring    = make_scorer(roc_auc_score,\n",
    "#                                             needs_threshold = False))\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# knn_cv.fit(X_scaled_df, ac_target)\n",
    "\n",
    "# # PREDICT step is not needed\n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", knn_cv.best_params_)\n",
    "# print(\"Tuned Training AUC:\", knn_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Tuned Training ACCURACY: 0.7423\n",
      "KNN Tuned Testing  ACCURACY: 0.7002\n",
      "KNN Train-Test Gap   : 0.0421\n",
      "\n",
      "True Negatives : 62\n",
      "False Positives: 94\n",
      "False Negatives: 52\n",
      "True Positives : 279\n",
      "\n",
      "KNN Tuned AUC Score : 0.6202\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# copy/pasting in the best_estimator_ results\n",
    "# to avoid running another RandomizedSearch\n",
    "knn_tuned = KNeighborsClassifier(algorithm = 'brute', leaf_size = 20,\n",
    "                                 n_neighbors = 10, p = 8)\n",
    "\n",
    "\n",
    "# FITTING the model object\n",
    "knn_tuned_fit = knn_tuned.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "knn_tuned_pred = knn_tuned_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('KNN Tuned Training ACCURACY:', knn_tuned.score(X_train, y_train).round(4))\n",
    "print('KNN Tuned Testing  ACCURACY:', knn_tuned.score(X_test, y_test).round(4))\n",
    "\n",
    "# saving scoring data for future use\n",
    "knn_tuned_train_score = knn_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "knn_tuned_test_score  = knn_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('KNN Train-Test Gap   :', abs(knn_tuned_train_score - \\\n",
    "                                       knn_tuned_test_score).round(4))\n",
    "\n",
    "knn_tuned_gap = abs(knn_tuned_train_score - knn_tuned_test_score).round(4)\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "knn_tn, \\\n",
    "knn_fp, \\\n",
    "knn_fn, \\\n",
    "knn_tp = confusion_matrix(y_true = y_test, y_pred = knn_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {knn_tn}\n",
    "False Positives: {knn_fp}\n",
    "False Negatives: {knn_fn}\n",
    "True Positives : {knn_tp}\n",
    "\"\"\")\n",
    "\n",
    "print('KNN Tuned AUC Score :', roc_auc_score(y_true  = y_test,\n",
    "                                             y_score = knn_tuned_pred).round(4))\n",
    "\n",
    "knn_auc_score = roc_auc_score(y_true  = y_test, y_score = knn_tuned_pred).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full One 48.31%\n",
    "# KNeighborsClassifier(leaf_size=35, n_neighbors=16, p=1)\n",
    "\n",
    "# Full Two 47.92%\n",
    "# KNeighborsClassifier(algorithm='ball_tree', leaf_size=45, n_neighbors=6, p=1)\n",
    "\n",
    "# Sig 1 54.86%\n",
    "# KNeighborsClassifier(algorithm='brute', leaf_size=25, n_neighbors=4, p=4)\n",
    "\n",
    "# Sig 2 60.37%\n",
    "# KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_neighbors=6, p=1)\n",
    "\n",
    "# Sig 3 55.23% - significant\n",
    "# KNeighborsClassifier(n_neighbors=14, p=1)\n",
    "\n",
    "# Sig 4 56.84%\n",
    "# KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_neighbors=4, p=4)\n",
    "\n",
    "# Sig 5 51.01%\n",
    "# KNeighborsClassifier(leaf_size=50, n_neighbors=4, p=1)\n",
    "\n",
    "# Sig 6 56.42%\n",
    "# KNeighborsClassifier(leaf_size=65, n_neighbors=4, p=1)\n",
    "\n",
    "# Sig 7 62.02% \n",
    "# KNeighborsClassifier(algorithm='brute', leaf_size=20, n_neighbors=10, p=8)\n",
    "\n",
    "# Sig 8 56.77%\n",
    "# KNeighborsClassifier(algorithm='brute', leaf_size=45, n_neighbors=6, p=1)\n",
    "\n",
    "# Sig 9 59.19%\n",
    "# KNeighborsClassifier(algorithm='kd_tree', leaf_size=50, n_neighbors=8, p=1)\n",
    "\n",
    "# Sig 10 52.59%\n",
    "# KNeighborsClassifier(algorithm='kd_tree', leaf_size=70, n_neighbors=4, p=1,\n",
    "#                      weights='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "ac_data   =  ac_dataset.loc[ : , candidate_dict['logit_sig_10']]\n",
    "ac_target =  ac_dataset.loc[ : , 'cross_sell_success']\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            ac_data,\n",
    "            ac_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = ac_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # declaring a hyperparameter space\n",
    "# estimator_space  = np.arange(200, 500, 50)\n",
    "# criterion_space  = ['gini', 'entropy']\n",
    "# depth_space = np.arange(1, 9, 1)\n",
    "# sample_split_space = np.arange(2, 10, 2)\n",
    "# leaf_space       = np.arange(1, 10, 1)\n",
    "# features_space   = [\"auto\", \"sqrt\", \"log2\"]\n",
    "# bootstrap_space  = [True, False]\n",
    "# warm_start_space = [True, False]\n",
    "\n",
    "\n",
    "# # creating a hyperparameter grid\n",
    "# param_grid = {'n_estimators'     : estimator_space,\n",
    "#               'min_samples_leaf' : leaf_space,\n",
    "#               'criterion'        : criterion_space,\n",
    "#               'bootstrap'        : bootstrap_space,\n",
    "#               'min_samples_split': sample_split_space,\n",
    "#               'warm_start'       : warm_start_space,\n",
    "#               'max_depth'        : depth_space,\n",
    "#               'max_features'     : features_space}\n",
    "\n",
    "\n",
    "# # INSTANTIATING the model object without hyperparameters\n",
    "# forest_grid = RandomForestClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# # GridSearchCV object\n",
    "# forest_cv = RandomizedSearchCV(estimator           = forest_grid,\n",
    "#                                param_distributions = param_grid,\n",
    "#                                cv         = 3,\n",
    "#                                n_iter     = 500,\n",
    "#                                scoring    = make_scorer(roc_auc_score,\n",
    "#                                             needs_threshold = False))\n",
    "\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# forest_cv.fit(ac_data, ac_target)\n",
    "\n",
    "\n",
    "# # PREDICT step is not needed\n",
    "\n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", forest_cv.best_params_)\n",
    "# print(\"Tuned Training AUC:\", forest_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest Tuned Training ACCURACY: 0.8547\n",
      "Forest Tuned Testing  ACCURACY: 0.8727\n",
      "Forest Train-Test Gap   : 0.018\n",
      "\n",
      "True Negatives : 99\n",
      "False Positives: 57\n",
      "False Negatives: 5\n",
      "True Positives : 326\n",
      "\n",
      "Forest Tuned AUC Score        : 0.8098\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# copy/pasting in the best_estimator_ results\n",
    "# to avoid running another RandomizedSearch\n",
    "forest_tuned = RandomForestClassifier(bootstrap=False, max_depth=8, max_features='sqrt',\n",
    "                        min_samples_split=4, n_estimators=350, random_state=219)\n",
    "\n",
    "\n",
    "# FITTING the model object\n",
    "forest_tuned_fit = forest_tuned.fit(ac_data, ac_target)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "forest_tuned_pred = forest_tuned_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Forest Tuned Training ACCURACY:', forest_tuned.score(X_train, y_train).round(4))\n",
    "print('Forest Tuned Testing  ACCURACY:', forest_tuned.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "forest_tuned_train_score = forest_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "forest_tuned_test_score  = forest_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Forest Train-Test Gap   :', abs(forest_tuned_train_score - \\\n",
    "                                       forest_tuned_test_score).round(4))\n",
    "\n",
    "forest_tuned_gap = abs(forest_tuned_train_score - forest_tuned_test_score).round(4)\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "rand_forest_tn, \\\n",
    "rand_forest_fp, \\\n",
    "rand_forest_fn, \\\n",
    "rand_forest_tp = confusion_matrix(y_true = y_test, y_pred = forest_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {rand_forest_tn}\n",
    "False Positives: {rand_forest_fp}\n",
    "False Negatives: {rand_forest_fn}\n",
    "True Positives : {rand_forest_tp}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "forest_tuned_auc = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = forest_tuned_pred).round(4) # auc\n",
    "\n",
    "\n",
    "print('Forest Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                                       y_score = forest_tuned_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full One 77.26% - significant\n",
    "# RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=8,\n",
    "#                        max_features='sqrt', min_samples_leaf=3,\n",
    "#                        min_samples_split=8, n_estimators=450, random_state=219)\n",
    "\n",
    "# Full Two 76.64% - significant\n",
    "# RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=8,\n",
    "#                        min_samples_leaf=6, n_estimators=250, random_state=219)\n",
    "\n",
    "# Sig 1 71.45%\n",
    "# RandomForestClassifier(bootstrap=False, max_depth=8, max_features='sqrt',\n",
    "#                        min_samples_leaf=5, min_samples_split=8,\n",
    "#                        n_estimators=480, random_state=219, warm_start=True)\n",
    "\n",
    "# Sig 2 73.32% significant\n",
    "# RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=8,\n",
    "#                        max_features='log2', min_samples_leaf=7,\n",
    "#                        min_samples_split=4, n_estimators=300, random_state=219)\n",
    "\n",
    "# Sig 3 73.83% - significant\n",
    "# RandomForestClassifier(bootstrap=False, max_depth=8, min_samples_leaf=4,\n",
    "#                        n_estimators=300, random_state=219)\n",
    "\n",
    "# Sig 4 68.74% significant\n",
    "# RandomForestClassifier(max_depth=8, max_features='sqrt', min_samples_leaf=6,\n",
    "#                        min_samples_split=6, n_estimators=200, random_state=219)\n",
    "\n",
    "# Sig 5 66.68% significant\n",
    "# RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=8,\n",
    "#                        max_features='sqrt', min_samples_leaf=9,\n",
    "#                        min_samples_split=4, n_estimators=500, random_state=219)\n",
    "\n",
    "# Sig 6 66.91% significant\n",
    "# RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=8,\n",
    "#                        max_features='sqrt', min_samples_leaf=8,\n",
    "#                        min_samples_split=8, n_estimators=600, random_state=219)\n",
    "\n",
    "# Sig 7 66.49% - significant\n",
    "# RandomForestClassifier(criterion='entropy', max_depth=7, max_features='log2',\n",
    "#                        n_estimators=750, random_state=219)\n",
    "\n",
    "# Sig 8 72.19% significant\n",
    "# RandomForestClassifier(bootstrap=False, max_depth=8, max_features='sqrt',\n",
    "#                        min_samples_leaf=2, n_estimators=500, random_state=219)\n",
    "\n",
    "# Sig 9 67.08% significant\n",
    "# RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=7,\n",
    "#                        max_features='sqrt', min_samples_leaf=3,\n",
    "#                        min_samples_split=8, n_estimators=1300,\n",
    "#                        random_state=219)\n",
    "\n",
    "# Sig 10 80.98% significant\n",
    "# RandomForestClassifier(bootstrap=False, max_depth=8, max_features='sqrt',\n",
    "#                        min_samples_split=4, n_estimators=350, random_state=219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################\n",
    "# # plot_feature_importances\n",
    "# ########################################\n",
    "# def plot_feature_importances(model, train, export = False):\n",
    "#     \"\"\"\n",
    "#     Plots the importance of features from a CART model.\n",
    "    \n",
    "#     PARAMETERS\n",
    "#     ----------\n",
    "#     model  : CART model\n",
    "#     train  : explanatory variable training data\n",
    "#     export : whether or not to export as a .png image, default False\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # declaring the number\n",
    "#     n_features = train.shape[1]\n",
    "    \n",
    "#     # setting plot window\n",
    "#     fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "#     plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "#     plt.yticks(pd.np.arange(n_features), train.columns)\n",
    "#     plt.xlabel(\"Feature importance\")\n",
    "#     plt.ylabel(\"Feature\")\n",
    "    \n",
    "#     if export == True:\n",
    "#         plt.savefig('./analysis_images/Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plotting feature importances\n",
    "# plot_feature_importances(forest_tuned_fit,\n",
    "#                          train = X_train,\n",
    "#                          export = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosted Machines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "ac_data   =  ac_dataset.loc[ : , candidate_dict['logit_sig_10']]\n",
    "ac_target =  ac_dataset.loc[ : , 'cross_sell_success']\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            ac_data,\n",
    "            ac_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = ac_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # declaring a hyperparameter space\n",
    "# estimator_space     = np.arange(100, 500, 100)\n",
    "# depth_space         = np.arange(1, 9, 1)\n",
    "# max_features_space  = ['auto', 'sqrt', 'log2']\n",
    "# loss_space          = ['deviance', 'exponential']\n",
    "# criterion_space     = ['friedman_mse', 'mse', 'mae']\n",
    "# min_split_space     = np.arange(2, 500, 100)\n",
    "# warm_start_space    = [True, False]\n",
    "# learn_space         = np.arange(0.1, 2.0, 0.1)\n",
    "\n",
    "# # creating a hyperparameter grid\n",
    "# param_grid = {'max_depth'     : depth_space,\n",
    "#               'n_estimators'  : estimator_space,\n",
    "#               'loss'          : loss_space,\n",
    "#               'criterion'     : criterion_space,\n",
    "#               'max_features'  : max_features_space,\n",
    "#               'warm_start'    : warm_start_space,\n",
    "#               'learning_rate' : learn_space}\n",
    "\n",
    "# # INSTANTIATING the model object without hyperparameters\n",
    "# full_gbm_grid = GradientBoostingClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# # GridSearchCV object\n",
    "# full_gbm_cv = RandomizedSearchCV(estimator     = full_gbm_grid,\n",
    "#                            param_distributions = param_grid,\n",
    "#                            cv                  = 3,\n",
    "#                            n_iter              = 200,\n",
    "#                            random_state        = 219,\n",
    "#                            scoring             = make_scorer(roc_auc_score,\n",
    "#                                                  needs_threshold = False))\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# full_gbm_cv.fit(ac_data, ac_target)\n",
    "\n",
    "# # PREDICT step is not needed\n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", full_gbm_cv.best_params_)\n",
    "# print(\"Tuned Training AUC:\", full_gbm_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7779\n",
      "Testing ACCURACY : 0.8008\n",
      "Forest Train-Test Gap   : 0.0229\n",
      "\n",
      "True Negatives : 84\n",
      "False Positives: 72\n",
      "False Negatives: 25\n",
      "True Positives : 306\n",
      "\n",
      "AUC Score        : 0.7315\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "gbm_tuned =  GradientBoostingClassifier(criterion='mse', loss='exponential', max_depth=2,\n",
    "                            max_features='sqrt', n_estimators=200,\n",
    "                            random_state=219)\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "gbm_tuned_fit = gbm_tuned.fit(ac_data, ac_target)\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "gbm_tuned_pred = gbm_tuned_fit.predict(X_test)\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', gbm_tuned_fit.score(X_train, y_train).round(4))\n",
    "print('Testing ACCURACY :', gbm_tuned_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_gbm_train_score = gbm_tuned_fit.score(X_train, y_train).round(4) # accuracy\n",
    "full_gbm_test_score  = gbm_tuned_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Forest Train-Test Gap   :', abs(full_gbm_train_score - \\\n",
    "                                       full_gbm_test_score).round(4))\n",
    "\n",
    "full_gbm_gap = abs(full_gbm_train_score - full_gbm_test_score).round(4)\n",
    "\n",
    "# unpacking the confusion matrix\n",
    "gbm_tuned_tn, \\\n",
    "gbm_tuned_fp, \\\n",
    "gbm_tuned_fn, \\\n",
    "gbm_tuned_tp = confusion_matrix(y_true = y_test, \n",
    "                                y_pred = gbm_tuned_pred).ravel()\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {gbm_tuned_tn}\n",
    "False Positives: {gbm_tuned_fp}\n",
    "False Negatives: {gbm_tuned_fn}\n",
    "True Positives : {gbm_tuned_tp}\n",
    "\"\"\")\n",
    "\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                  y_score = gbm_tuned_pred).round(4))\n",
    "\n",
    "# saving the AUC score\n",
    "gbm_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = gbm_tuned_pred).round(4) # auce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full One 66.6% - significant\n",
    "# GradientBoostingClassifier(loss='exponential', max_depth=1, max_features='auto',\n",
    "#                            n_estimators=300, random_state=219, warm_start=True)\n",
    "\n",
    "# Full Two 67.89% - significant\n",
    "# GradientBoostingClassifier(loss='exponential', max_depth=1, max_features='auto',\n",
    "#                            n_estimators=300, random_state=219, warm_start=True)\n",
    "\n",
    "# Sig 1 62.09% - significant\n",
    "# GradientBoostingClassifier(criterion='mae', learning_rate=1.2000000000000002,\n",
    "#                            loss='exponential', max_depth=1, max_features='auto',\n",
    "#                            random_state=219, warm_start=True)\n",
    "\n",
    "# Sig 2 68.22% - significant\n",
    "# GradientBoostingClassifier(criterion='mse', learning_rate=0.5,\n",
    "#                            loss='exponential', max_depth=1, max_features='sqrt',\n",
    "#                            random_state=219, warm_start=True)\n",
    "\n",
    "# Sig 3 62.09% - significant\n",
    "# GradientBoostingClassifier(criterion='mae', learning_rate=1.2000000000000002,\n",
    "#                            loss='exponential', max_depth=1, max_features='auto',\n",
    "#                            random_state=219, warm_start=True)\n",
    "\n",
    "# Sig 4 62.09% - significant\n",
    "# GradientBoostingClassifier(criterion='mae', learning_rate=1.2000000000000002,\n",
    "#                            loss='exponential', max_depth=1, max_features='auto',\n",
    "#                            random_state=219, warm_start=True)\n",
    "\n",
    "# Sig 5 66.64% - significant\n",
    "# GradientBoostingClassifier(learning_rate=0.30000000000000004,\n",
    "#                            loss='exponential', max_depth=1, max_features='log2',\n",
    "#                            n_estimators=400, random_state=219, warm_start=True)\n",
    "\n",
    "# Sig 6 65.41% - significant\n",
    "# GradientBoostingClassifier(learning_rate=1.9000000000000001, loss='exponential',\n",
    "#                            max_depth=1, max_features='auto', random_state=219)\n",
    "\n",
    "# Sig 7 63.38% - significant\n",
    "# GradientBoostingClassifier(criterion='mse', learning_rate=0.5,\n",
    "#                            loss='exponential', max_depth=1, max_features='sqrt',\n",
    "#                            random_state=219, warm_start=True)\n",
    "\n",
    "# Sig 8 64.02% - significant\n",
    "# GradientBoostingClassifier(criterion='mse', learning_rate=0.2, max_depth=1,\n",
    "#                            max_features='auto', n_estimators=400,\n",
    "#                            random_state=219)\n",
    "\n",
    "# Sig 9 70.43% - significant\n",
    "# GradientBoostingClassifier(criterion='mse', loss='exponential', max_depth=2,\n",
    "#                            max_features='sqrt', n_estimators=200,\n",
    "#                            random_state=219)\n",
    "\n",
    "# Sig 10 73.15% - significant\n",
    "# GradientBoostingClassifier(criterion='mse', loss='exponential', max_depth=2,\n",
    "#                            max_features='sqrt', n_estimators=200,\n",
    "#                            random_state=219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Train-Test Gap</th>\n",
       "      <th>Confusion Matrix (TN, FP, FN, TP)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6777</td>\n",
       "      <td>0.7539</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>(71, 85, 33, 298)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classification Trees</td>\n",
       "      <td>0.7320</td>\n",
       "      <td>0.7402</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>(95, 61, 48, 283)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K-Nearest Neighbors Classification</td>\n",
       "      <td>0.6202</td>\n",
       "      <td>0.7423</td>\n",
       "      <td>0.7002</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>(62, 94, 52, 279)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest [CHOSEN]</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.8547</td>\n",
       "      <td>0.8727</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>(99, 57, 5, 326)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosted Models</td>\n",
       "      <td>0.7315</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.8008</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>(84, 72, 25, 306)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model Name  AUC Score  Training Accuracy  Testing Accuracy  Train-Test Gap Confusion Matrix (TN, FP, FN, TP)\n",
       "0                 Logistic Regression     0.6777             0.7539            0.7577          0.0038                 (71, 85, 33, 298)\n",
       "1                Classification Trees     0.7320             0.7402            0.7762          0.0360                 (95, 61, 48, 283)\n",
       "2  K-Nearest Neighbors Classification     0.6202             0.7423            0.7002          0.0421                 (62, 94, 52, 279)\n",
       "3              Random Forest [CHOSEN]     0.8098             0.8547            0.8727          0.0180                  (99, 57, 5, 326)\n",
       "4             Gradient Boosted Models     0.7315             0.7779            0.8008          0.0229                 (84, 72, 25, 306)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Name'        : ['Logistic Regression',\n",
    "                           'Classification Trees', \n",
    "                           'K-Nearest Neighbors Classification',\n",
    "                           'Random Forest [CHOSEN]',\n",
    "                           'Gradient Boosted Models'],\n",
    "           \n",
    "    'AUC Score'         : [logreg_auc_score, \n",
    "                           tree_tuned_auc, \n",
    "                           knn_auc_score,\n",
    "                           forest_tuned_auc,\n",
    "                           gbm_tuned_auc],\n",
    "    \n",
    "    'Training Accuracy' : [logreg_train_score, \n",
    "                           tree_tuned_train_score,\n",
    "                           knn_tuned_train_score,\n",
    "                           forest_tuned_train_score,\n",
    "                           full_gbm_train_score],\n",
    "           \n",
    "    'Testing Accuracy'  : [logreg_test_score, \n",
    "                           tree_tuned_test_score,\n",
    "                           knn_tuned_test_score,\n",
    "                           forest_tuned_test_score,\n",
    "                           full_gbm_test_score],\n",
    "    \n",
    "    'Train-Test Gap'    : [logreg_test_gap,\n",
    "                           tree_tuned_test_gap,\n",
    "                           knn_tuned_gap,\n",
    "                           forest_tuned_gap,\n",
    "                           full_gbm_gap],\n",
    "\n",
    "    'Confusion Matrix (TN, FP, FN, TP)'  : [(logreg_tn, logreg_fp, logreg_fn, logreg_tp),\n",
    "                           (tree_tuned_tn, tree_tuned_fp, tree_tuned_fn, tree_tuned_tp),\n",
    "                           (knn_tn, knn_fp, knn_fn, knn_tp),\n",
    "                           (rand_forest_tn, rand_forest_fp, rand_forest_fn, rand_forest_tp),                 \n",
    "                           (gbm_tuned_tn, gbm_tuned_fp, gbm_tuned_fn, gbm_tuned_tp)]}\n",
    "\n",
    "\n",
    "# converting model_performance into a DataFrame\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chosen model is Random Forest (Classification) with variables:\n",
      "['total_meals_ordered', 'log_contacts_w_customer_service', 'mobile_number', 'cancellations_before_noon', 'tastes_and_preferences', 'mobile_logins', 'weekly_plan', 'log_avg_prep_vid_time', 'has_master_classes_attended', 'log_pc_logins', 'email_junk', 'procastinator', 'email_professional', 'log_product_categories_viewed', 'working', 'active_pc_user', 'common_user', 'weekend_fighter', 'number_of_names']\n"
     ]
    }
   ],
   "source": [
    "print('The chosen model is Random Forest (Classification) with variables:')\n",
    "print(candidate_dict['logit_sig_10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ 12.565959930419922 seconds ___\n"
     ]
    }
   ],
   "source": [
    "print(\"___ %s seconds ___\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
